{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tool for analyzing and processing the generated site index file\n",
    "\n",
    "The last step converts the limited index object to a set of tasks that will be executed by the scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import json\n",
    "import copy\n",
    "import time\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants and globals\n",
    "filepath_index = \"data/scraped/pangoly/index.json\"\n",
    "print(f\"Loading index from {filepath_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load index\n",
    "index = json.loads(open(filepath_index).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the dictionary with categories count, number of skipped categories and number of products in each category\n",
    "def get_categories_info(index_obj: dict):\n",
    "\tcategories_info = {}\n",
    "\tcategories_info[\"categories_count\"] = len(index_obj[\"categories\"])\n",
    "\tcategories_info[\"categories_skipped_count\"] = 0\n",
    "\tcategories_info[\"categories_products_count\"] = {}\n",
    "\tcategories_info[\"products_total_count\"] = 0\n",
    "\tfor category_key, category in index_obj[\"categories\"].items():\n",
    "\t\tif category[\"products\"] == None:\n",
    "\t\t\t# None means we didn't walk this category\n",
    "\t\t\tcategories_info[\"categories_skipped_count\"] += 1\n",
    "\t\telse:\n",
    "\t\t\tproducts = category[\"products\"]\n",
    "\t\t\tcategories_info[\"products_total_count\"] += len(products)\n",
    "\t\t\tcategories_info[\"categories_products_count\"][category_key] = len(products)\n",
    "\treturn categories_info\n",
    "\n",
    "# Print number of products in each category\n",
    "def print_category_info(index_obj: dict, include_skipped: bool = False):\n",
    "\tcategories_info = get_categories_info(index_obj)\n",
    "\tprint(f\"Products indexed: {categories_info['products_total_count']}\")\n",
    "\tprint(f\"Categories: {categories_info['categories_count']} ({categories_info['categories_skipped_count']} of those are skipped)\")\n",
    "\tfor category_key, category in index_obj[\"categories\"].items():\n",
    "\t\tif category[\"products\"] == None:\n",
    "\t\t\t# None means we didn't walk this category\n",
    "\t\t\tif include_skipped:\n",
    "\t\t\t\tprint(f\" - {category_key} ({category['name']}): (skipped)\")\n",
    "\t\telse:\n",
    "\t\t\tproducts = category[\"products\"]\n",
    "\t\t\tprint(f\" - {category_key} ({category['name']}): {len(products)} ({round(len(products) / categories_info['products_total_count'] * 100, 2)}%)\")\n",
    "\t# Print products info\n",
    "\n",
    "\n",
    "\n",
    "print_category_info(index, include_skipped=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get non-destructive deep copy of the index object with a limited list of products for each category (pick first n)\n",
    "def limit_categories(index_obj: dict, limit: int) -> dict:\n",
    "\tindex_limited = copy.deepcopy(index_obj)\n",
    "\tfor category_key, category in index_limited[\"categories\"].items():\n",
    "\t\tif category[\"products\"] != None:\n",
    "\t\t\tproducts_slice = dict(list(category[\"products\"].items())[:limit])\n",
    "\t\t\tindex_limited[\"categories\"][category_key][\"products\"] = products_slice\n",
    "\treturn index_limited\n",
    "\t\n",
    "limit = 500\n",
    "print(f\"Limiting categories to {limit} products each\")\n",
    "index_limited = limit_categories(index, limit)\n",
    "print_category_info(index_limited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rough estimates of scraping time for the whole index depending on different parameters\n",
    "# One products scraped by being run as a task designated to a separate worker process by a scheduler\n",
    "def print_scraping_estimates(index_obj: dict, seconds_per_task: float, max_workers: int):\n",
    "\tcategories_info = get_categories_info(index_obj)\n",
    "\tproducts_total_count = categories_info[\"products_total_count\"]\n",
    "\ttime_total_sequential = products_total_count * seconds_per_task\n",
    "\ttime_total_parallel = time_total_sequential / max_workers\n",
    "\tprint(f\"Scraping estimates for {products_total_count} products:\")\n",
    "\tprint(f\" - sequential: {round(time_total_sequential / 60, 2)} minutes ({round(time_total_sequential / 60 / 60, 2)} hours)\")\n",
    "\tprint(f\" - parallel ({max_workers} workers): {round(time_total_parallel / 60, 2)} minutes ({round(time_total_parallel / 60 / 60, 2)} hours)\")\n",
    "\n",
    "seconds_per_task = 65.7 # 65.7\n",
    "max_workers = 30 # 30\n",
    "print(f\"Scraping estimates for {seconds_per_task} seconds per task and {max_workers} workers:\")\n",
    "print(f\"index:\")\n",
    "print_scraping_estimates(index, seconds_per_task, max_workers)\n",
    "print(f\"index_limited:\")\n",
    "print_scraping_estimates(index_limited, seconds_per_task, max_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts index object to a list of tasks that can be consumed by the scheduler\n",
    "# Example of a task:\n",
    "# {\n",
    "# \t\"metadata\": {\n",
    "# \t\t\"handle\": \"handle_1\",\n",
    "# \t\t\"retries\": 1,\n",
    "# \t\t\"can_handle_output\": false,\n",
    "# \t\t\"can_handle_error\": false\n",
    "# \t},\n",
    "# \t\"args\": [\n",
    "# \t\t\"python\",\n",
    "# \t\t\"src/py/utils/scheduler/dummy_task.py\"\n",
    "# \t]\n",
    "# }\n",
    "\n",
    "def convert_to_tasks(index_obj: dict) -> list:\n",
    "\ttasks = []\n",
    "\tfor category_key, category in index_obj[\"categories\"].items():\n",
    "\t\tif category[\"products\"] != None:\n",
    "\t\t\tfor product_key, product in category[\"products\"].items():\n",
    "\t\t\t\ttask = {\n",
    "\t\t\t\t\t\"metadata\": {\n",
    "\t\t\t\t\t\t\"handle\": f\"{category_key}/{product_key}\",\n",
    "\t\t\t\t\t\t\"retries\": 1,\n",
    "\t\t\t\t\t\t\"can_handle_output\": False,\n",
    "\t\t\t\t\t\t\"can_handle_error\": False\n",
    "\t\t\t\t\t},\n",
    "\t\t\t\t\t\"args\": [\n",
    "\t\t\t\t\t\t\"python\",\n",
    "\t\t\t\t\t\t\"src/py/scraping/pangoly/scraper_worker.py\",\n",
    "\t\t\t\t\t\tproduct_key\n",
    "\t\t\t\t\t]\n",
    "\t\t\t\t}\n",
    "\t\t\t\ttasks.append(task)\n",
    "\treturn tasks\n",
    "\n",
    "tasks = convert_to_tasks(index_limited)\n",
    "print(f\"Converted index to {len(tasks)} tasks\")\n",
    "print(f\"Example task:\")\n",
    "print(json.dumps(tasks[0], indent=2))\n",
    "\n",
    "# Save tasks to file (same filepath as index but with .tasks.json extension)\n",
    "filepath_tasks = filepath_index.replace(\".json\", \".tasks.json\")\n",
    "print(f\"Saving tasks to {filepath_tasks}\")\n",
    "time_start = time.time()\n",
    "with open(filepath_tasks, \"w\") as f:\n",
    "\tf.write(json.dumps(tasks, indent=2))\n",
    "time_end = time.time()\n",
    "print(f\"Saved tasks in {round(time_end - time_start, 2)} seconds\")\n",
    "print(f\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make a parameter that describes number of repetitions for category {\"cpu\": 2, \"vga\": 1} means take 2 cpus and 1 vga per loop\n",
    "def convert_to_tasks_round_robin(index_obj: dict, allowed_categories: list, total_limit: int = -1, task_dict: Optional[dict] = None) -> dict:\n",
    "\t'''\n",
    "\t\tConverts index object to a list of tasks for allowed categories in a round-robin fashion\n",
    "\t\tKeys are task handles, values are tasks - for easier merging\n",
    "\t\ttask_dict is used for not adding duplicate tasks\n",
    "\t'''\n",
    "\ttasks = {}\n",
    "\tif task_dict != None:\n",
    "\t\ttasks = task_dict\n",
    "\t\ttotal_limit += len(tasks)\n",
    "\tcategories = {}\n",
    "\tindex_obj_copy = copy.deepcopy(index_obj)\n",
    "\tfor category in allowed_categories:\n",
    "\t\tcategories[category] = index_obj_copy[\"categories\"][category]\n",
    "\twhile len(categories) > 0:\n",
    "\t\tif len(tasks) >= total_limit and total_limit != -1:\n",
    "\t\t\tbreak\n",
    "\t\tfor category_key, category in categories.items():\n",
    "\t\t\tif len(tasks) >= total_limit and total_limit != -1:\n",
    "\t\t\t\tbreak\n",
    "\t\t\tproducts = category[\"products\"]\n",
    "\t\t\tif len(products) == 0:\n",
    "\t\t\t\tdel categories[category_key]\n",
    "\t\t\t\tbreak\n",
    "\t\t\t# get product from the end of the list (get oldest product first)\n",
    "\t\t\tproduct_key, product = products.popitem()\n",
    "\t\t\t# get product from the start of the list (get newest product first)\n",
    "\t\t\t# product_key, product = products.pop(list(products.keys())[0])\n",
    "\t\t\ttask = {\n",
    "\t\t\t\t\t\"metadata\": {\n",
    "\t\t\t\t\t\t\"handle\": f\"{category_key}/{product_key}\",\n",
    "\t\t\t\t\t\t\"retries\": 1,\n",
    "\t\t\t\t\t\t\"can_handle_output\": False,\n",
    "\t\t\t\t\t\t\"can_handle_error\": False\n",
    "\t\t\t\t\t},\n",
    "\t\t\t\t\t\"args\": [\n",
    "\t\t\t\t\t\t\"python\",\n",
    "\t\t\t\t\t\t\"src/py/scraping/pangoly/scraper_worker.py\",\n",
    "\t\t\t\t\t\tproduct_key\n",
    "\t\t\t\t\t]\n",
    "\t\t\t\t}\n",
    "\t\t\tif task[\"metadata\"][\"handle\"] not in tasks:\n",
    "\t\t\t\ttasks[task[\"metadata\"][\"handle\"]] = task\n",
    "\treturn tasks\n",
    "\n",
    "def split_list(lst: list, n: int) -> list:\n",
    "\tsize = len(lst) // n\n",
    "\tremainder = len(lst) % n\n",
    "\tsplits = [size + 1] * remainder + [size] * (n - remainder)\n",
    "\treturn [lst[sum(splits[:i]):sum(splits[:i+1])] for i in range(n)]\n",
    "\n",
    "# l = split_list([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 3) # produces [[1, 2, 3, 4], [5, 6, 7], [8, 9, 10]]\n",
    "\n",
    "def save_splits(splits: list):\n",
    "\t'''\n",
    "\t\tSaves a list of splits to a number of files\n",
    "\t'''\n",
    "\tfor i, split in enumerate(splits):\n",
    "\t\tfilepath = filepath_index.replace(\".json\", f\"-rr-split_{i}.tasks.json\")\n",
    "\t\tprint(f\"Saving split '{i}' to '{filepath}'\")\n",
    "\t\twith open(filepath, \"w\") as f:\n",
    "\t\t\tf.write(json.dumps(split, indent=2))\n",
    "\t\tprint(f\"Done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# task_dict = convert_to_tasks_round_robin(index_limited, [\"cpu\", \"vga\"], 4)\n",
    "# task_dict = convert_to_tasks_round_robin(index_limited, [\"ram\", \"ssd\"], 4, task_dict)\n",
    "# task_dict = convert_to_tasks_round_robin(index_limited, [\"cpu\", \"vga\"], 4, task_dict)\n",
    "\n",
    "# Dictionary in steps\n",
    "task_dict = convert_to_tasks_round_robin(index_limited, [\"cpu\", \"vga\"], 200)\n",
    "task_dict = convert_to_tasks_round_robin(index_limited, [\"ram\", \"ssd\"], 200, task_dict)\n",
    "task_dict = convert_to_tasks_round_robin(index_limited, [\"cpu\", \"vga\"], 800, task_dict)\n",
    "task_dict = convert_to_tasks_round_robin(index_limited, [\"ram\", \"ssd\"], 800, task_dict)\n",
    "\n",
    "# # print handles\n",
    "print(\"Task handles:\")\n",
    "for i, task in enumerate(task_dict.values()):\n",
    "\tprint(f\" * {task['metadata']['handle']}\")\n",
    "\tif i >= 10:\n",
    "\t\tbreak\n",
    "\n",
    "# print counts by category\n",
    "print(\"Task counts by category:\")\n",
    "category_counts = {}\n",
    "for handle in task_dict.keys():\n",
    "\tcategory = handle.split(\"/\")[0]\n",
    "\tif category not in category_counts:\n",
    "\t\tcategory_counts[category] = 0\n",
    "\tcategory_counts[category] += 1\n",
    "for category, count in category_counts.items():\n",
    "\tprint(f\" * {category}: {count}\")\n",
    "\n",
    "# convert to tasklist\n",
    "task_list = list(task_dict.values())\n",
    "print(f\"Converted index to {len(task_list)} tasks (round-robin)\")\n",
    "print(f\"Example task:\")\n",
    "print(json.dumps(task_list[0], indent=2))\n",
    "\n",
    "# Save tasks to file (same filepath as index but with .tasks.json extension)\n",
    "filepath_tasks = filepath_index.replace(\".json\", \"-rr.tasks.json\")\n",
    "print(f\"Saving tasks to {filepath_tasks}\")\n",
    "time_start = time.time()\n",
    "with open(filepath_tasks, \"w\") as f:\n",
    "\tf.write(json.dumps(task_list, indent=2))\n",
    "time_end = time.time()\n",
    "print(f\"Saved tasks in {round(time_end - time_start, 2)} seconds\")\n",
    "\n",
    "# Split tasks into 3 files (for multiple devices to run in parallel)\n",
    "splits = split_list(task_list, 3)\n",
    "print(\"Split sizes (tasks per split):\")\n",
    "for i, split in enumerate(splits):\n",
    "\tprint(f\" * {i}: {len(split)}\")\n",
    "print(\"Saving splits...\")\n",
    "save_splits(splits)\n",
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
