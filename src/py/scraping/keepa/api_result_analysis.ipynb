{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from typing import Tuple\n",
    "\n",
    "# Project imports\n",
    "sys.path.append(os.getcwd())\n",
    "from src.py.utils.trash import move_to_trash, get_trash_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_output_root = \"data/keepa/products/domains\"\n",
    "# domain_id: Amazon domain ID - Valid values: [ 1: com | 2: co.uk 231 | 3: de | 4: fr | 5: co.jp | 6: ca | 8: it | 9: es | 10: in | 11: com.mx ]\n",
    "domains_map = {\n",
    "\t\"1\": \"com\",\n",
    "\t\"2\": \"co.uk\",\n",
    "\t\"3\": \"de\",\n",
    "\t\"4\": \"fr\",\n",
    "\t\"5\": \"co.jp\",\n",
    "\t\"6\": \"ca\",\n",
    "\t\"8\": \"it\",\n",
    "\t\"9\": \"es\",\n",
    "\t\"10\": \"in\",\n",
    "\t\"11\": \"com.mx\",\n",
    "}\n",
    "domains = sorted(os.listdir(path_output_root))\n",
    "domains = {domain: {\"path\": os.path.join(path_output_root, domain)}for domain in domains}\n",
    "path_parsed_products = \"data/scraped/camel/parsed-products.json\"\n",
    "\n",
    "print(f\"Domains: {json.dumps(domains, indent=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_json_files(path):\n",
    "\tjson_files = []\n",
    "\tfor root, dirs, files in os.walk(path):\n",
    "\t\tfor file in files:\n",
    "\t\t\tif file.endswith('.json'):\n",
    "\t\t\t\tjson_files.append(os.path.join(root, file))\n",
    "\treturn json_files\n",
    "\n",
    "\n",
    "# json_files = get_all_json_files(path_output_root)\n",
    "# print(f\"Got {len(json_files)} json files\")\n",
    "\n",
    "for domain in domains:\n",
    "\tdomains[domain][\"json_files\"] = get_all_json_files(domains[domain][\"path\"]) # type: ignore\n",
    "\tprint(f\"Domain {domain} got {len(domains[domain]['json_files'])} json files\") # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_failed_files(json_files: list) -> list:\n",
    "\t\"\"\"\n",
    "\t:param json_files: list of json files\n",
    "\t:return: number of files with error and total number of files\n",
    "\t\"\"\"\n",
    "\tfailed_files = []\n",
    "\tfor i, file in enumerate(json_files):\n",
    "\t\tasin = file.split(\"/\")[-1].split(\".\")[0]\n",
    "\t\tprint(f\"Checking file {i+1}/{len(json_files)}: {asin}\", end=\"\\r\")\n",
    "\t\ttry:\n",
    "\t\t\twith open(file) as f:\n",
    "\t\t\t\tdata = json.load(f)\n",
    "\t\t\t\tif \"error\" in data:\n",
    "\t\t\t\t\tfailed_files.append((asin, data[\"error\"]))\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tfailed_files.append((asin, f\"File parsing error: {e}\"))\n",
    "\treturn failed_files\n",
    "\n",
    "# TODO: if you get failed files, create a function that returns\n",
    "#       a dictionary of unique errors as keys and a list of asins as values\n",
    "#       then sort the dictionary by the number of asins in the list\n",
    "def get_errors_dict(failed_files: list) -> dict:\n",
    "\t\"\"\"\n",
    "\t:param failed_files: list of tuples (asin, error)\n",
    "\t:return: dictionary of unique errors as keys and a list of asins as values\n",
    "\t\"\"\"\n",
    "\terrors_dict = {}\n",
    "\tfor asin, error in failed_files:\n",
    "\t\terror_short = error[:100]\n",
    "\t\tif error_short not in errors_dict:\n",
    "\t\t\terrors_dict[error_short] = [asin]\n",
    "\t\telse:\n",
    "\t\t\terrors_dict[error_short].append(asin)\n",
    "\treturn errors_dict\n",
    "\n",
    "def get_errors_dict_sorted(errors_dict: dict) -> dict:\n",
    "\t\"\"\"\n",
    "\t:param errors_dict: dictionary of unique errors as keys and a list of asins as values\n",
    "\t:return: sorted dictionary of unique errors as keys and a list of asins as values\n",
    "\t\"\"\"\n",
    "\terrors_dict_sorted = {k: v for k, v in sorted(errors_dict.items(), key=lambda item: len(item[1]), reverse=True)}\n",
    "\treturn errors_dict_sorted\n",
    "\n",
    "def get_errors_dict_sorted_counts(errors_dict_sorted: dict) -> dict:\n",
    "\t\"\"\"\n",
    "\t:param errors_dict_sorted: sorted dictionary of unique errors as keys and a list of asins as values\n",
    "\t:return: sorted dictionary of unique errors as keys and a list of asins as values\n",
    "\t\"\"\"\n",
    "\terrors_dict_sorted_counts = {}\n",
    "\tfor k, v in errors_dict_sorted.items():\n",
    "\t\terrors_dict_sorted_counts[k] = len(v)\n",
    "\treturn errors_dict_sorted_counts\n",
    "\n",
    "# failed_files = get_failed_files(json_files)\n",
    "# print(f\"Got {len(failed_files)} failed files\")\n",
    "# if len(failed_files) > 0:\n",
    "# \terrors_dict = get_errors_dict(failed_files)\n",
    "# \terrors_dict_sorted = get_errors_dict_sorted(errors_dict)\n",
    "# \terrors_dict_sorted_counts = get_errors_dict_sorted_counts(errors_dict_sorted)\n",
    "# \tprint(f\"Number of unique errors: {len(errors_dict_sorted_counts)}\")\n",
    "# \tprint(f\"Errors: {json.dumps(errors_dict_sorted_counts, indent=2)}\")\n",
    "\n",
    "print(\"Checking domains for failed files...\")\n",
    "for domain in domains:\n",
    "\tfailed_files = get_failed_files(domains[domain][\"json_files\"]) # type: ignore\n",
    "\tdomains[domain][\"failed_files\"] = failed_files # type: ignore\n",
    "\tprint(\"\")\n",
    "\tprint(f\"Domain {domain} got {len(failed_files)} failed files\") # type: ignore\n",
    "\tif len(failed_files) > 0:\n",
    "\t\terrors_dict = get_errors_dict(failed_files)\n",
    "\t\terrors_dict_sorted = get_errors_dict_sorted(errors_dict)\n",
    "\t\terrors_dict_sorted_counts = get_errors_dict_sorted_counts(errors_dict_sorted)\n",
    "\t\tdomains[domain][\"errors_dict_sorted\"] = errors_dict # type: ignore\n",
    "\t\tdomains[domain][\"errors_dict_sorted_counts\"] = errors_dict_sorted_counts # type: ignore\n",
    "\t\tprint(f\"Domain {domain} errors ({len(errors_dict_sorted_counts)} unique errors): {json.dumps(errors_dict_sorted_counts, indent=2)}\") # type: ignore\n",
    "\tprint(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_failed_files(failed_files: list):\n",
    "\t\"\"\"\n",
    "\t:param failed_files: list of tuples (asin, error)\n",
    "\t:return: None\n",
    "\t\"\"\"\n",
    "\tfor asin, error in failed_files:\n",
    "\t\tfilepath = f\"{path_output_root}/{asin}.json\"\n",
    "\t\ttrash_path_info = get_trash_path(filepath)\n",
    "\t\tprint(f\"Moving file '{filepath}' to trash '{trash_path_info}'\")\n",
    "\t\terr = move_to_trash(filepath)\n",
    "\t\tif err:\n",
    "\t\t\tprint(f\"  Error moving file '{filepath}' to trash '{trash_path_info}': {err}\")\n",
    "\t\telse:\n",
    "\t\t\tprint(f\"  OK\")\n",
    "\n",
    "\n",
    "# WARNING: uncomment the line below to delete failed files\n",
    "# delete_failed_files(failed_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_averages(json_files: list) -> Tuple[dict, dict]:\n",
    "\t'''\n",
    "\t\tReturns a dictionary with the average values of specific keys in the json files\n",
    "\t'''\n",
    "\t# Example json file:\n",
    "\t# {\n",
    "\t# \t\"tokensLeft\": 3,\n",
    "\t# \t\"refillRate\": 5,\n",
    "\t# \t\"tokenFlowReduction\": 0.0,\n",
    "\t# \t\"tokensConsumed\": 2,\n",
    "\t# \t\"processingTimeInMs\": 1,\n",
    "\t# }\n",
    "\taverages = {\n",
    "\t    \"tokensLeft\": 0.,  # should be around 5\n",
    "\t    \"refillRate\": 0.,  # should be around 5 tokens per minute\n",
    "\t    \"tokenFlowReduction\": 0.,  # should be 0\n",
    "\t    \"tokensConsumed\": 0.,  # should be around 2 tokens per request\n",
    "\t    \"processingTimeInMs\": 0.,  # should be around 500\n",
    "\t}\n",
    "\tsuccessfully_processed_files = 0\n",
    "\tfailed_files = {}\n",
    "\tfor i, json_file in enumerate(json_files):\n",
    "\t\tprint(f\"Processing file {i+1}/{len(json_files)}: {json_file}\", end=\"\\r\")\n",
    "\t\ttry:\n",
    "\t\t\twith open(json_file, 'r') as f:\n",
    "\t\t\t\tdata = json.load(f)\n",
    "\t\t\t\t# check if we have all keys first\n",
    "\t\t\t\tfor key in averages.keys():\n",
    "\t\t\t\t\tif key not in data:\n",
    "\t\t\t\t\t\traise Exception(f\"Key '{key}' not in file '{json_file}'\")\n",
    "\t\t\t\tfor key in averages.keys():\n",
    "\t\t\t\t\taverages[key] += data[key]\n",
    "\t\t\tsuccessfully_processed_files += 1\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint(f\"Error processing file '{json_file}': '{e}'\")\n",
    "\t\t\tfailed_files[json_file] = e\n",
    "\tfor key in averages.keys():\n",
    "\t\taverages[key] /= successfully_processed_files\n",
    "\t# assuming we always hit token limits and use average token refill rate for minute\n",
    "\trequests_per_minute = averages[\"refillRate\"] / averages[\"tokensConsumed\"]\n",
    "\taverages[\"requestsPerMinute\"] = requests_per_minute\n",
    "\treturn averages, failed_files\n",
    "\n",
    "\n",
    "# averages = get_averages(json_files)\n",
    "# print(f\"Averages: {json.dumps(averages, indent=2)}\")\n",
    "\n",
    "for domain in domains:\n",
    "\tprint(f\"Getting averages for domain {domain}...\")\n",
    "\taverages, failed_files = get_averages(domains[domain][\"json_files\"]) # type: ignore\n",
    "\tprint(\"\")\n",
    "\tdomains[domain][\"averages\"] = averages # type: ignore\n",
    "\tdomains[domain][\"failed_files_averages\"] = failed_files # type: ignore\n",
    "\tprint(f\"Domain {domain} averages: {json.dumps(averages, indent=2)}\") # type: ignore\n",
    "\tprint(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for domain in domains:\n",
    "\tprint(f\"Domain {domain} summary:\")\n",
    "\tprint(f\"{len(domains[domain]['failed_files_averages'])}/{len(domains[domain]['json_files'])} failed files\") # type: ignore\n",
    "\tprint(f\"Domain {domain} averages: {json.dumps(domains[domain]['averages'], indent=2)}\") # type: ignore\n",
    "\tprint(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_estimates(averages: dict,\n",
    "                       product_count: int) -> Tuple[float, float]:\n",
    "\t'''\n",
    "\t\tPrints time estimates\n",
    "\t'''\n",
    "\trequests_per_minute = averages[\"requestsPerMinute\"]\n",
    "\tseconds_needed = product_count / requests_per_minute * 60\n",
    "\tdays_needed = seconds_needed / 60 / 60 / 24\n",
    "\t# print(f\"Seconds needed: {seconds_needed}\")\n",
    "\t# print(f\"Days needed: {days_needed}\")\n",
    "\treturn seconds_needed, days_needed\n",
    "\n",
    "\n",
    "# print(\n",
    "#     f\"Current speed: {averages['requestsPerMinute']} requests per minute ({averages['requestsPerMinute'] * 60 * 24} per day)\"\n",
    "# )\n",
    "# print()\n",
    "# # products_count = 13884 * 3 # assuming we have 3 domains - US, UK, DE\n",
    "# # product_count equals to the number of products in the parsed-products.json file\n",
    "# parsed_products = {}\n",
    "# with open(path_parsed_products, 'r') as f:\n",
    "# \tparsed_products = json.load(f)\n",
    "# products_count = len(parsed_products)\n",
    "# domains = set([\"US\", \"UK\", \"DE\"])\n",
    "# products_count = products_count * len(domains)\n",
    "# products_left = products_count - len(json_files)\n",
    "# print(f\"Products total: {products_count}\")\n",
    "# seconds_total, _ = get_time_estimates(averages, products_count)\n",
    "# print()\n",
    "# print(f\"Products left: {products_left}\")\n",
    "# seconds_left, _ = get_time_estimates(averages, products_left)\n",
    "# print()\n",
    "# print(f\"Progress: {len(json_files) / products_count * 100:.2f}%\")\n",
    "\n",
    "for domain in domains:\n",
    "\tseconds_total, days_total = get_time_estimates(domains[domain][\"averages\"], len(domains[domain][\"json_files\"])) # type: ignore\n",
    "\tdomains[domain][\"seconds_total\"] = seconds_total # type: ignore\n",
    "\tdomains[domain][\"days_total\"] = days_total # type: ignore\n",
    "\tprint(f\"Domain {domain} took {seconds_total:.2f} seconds ({days_total:.2f} days) to fetch data for {len(domains[domain]['json_files'])} products\") # type: ignore\n",
    "\tprint(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_averages(averages: dict):\n",
    "\tprint(f\" - tokensLeft: {averages['tokensLeft']:.2f} (should be around 5)\")\n",
    "\tprint(f\" - refillRate: {averages['refillRate']:.2f} (should be around 5 tokens per minute)\")\n",
    "\tprint(f\" - tokenFlowReduction: {averages['tokenFlowReduction']:.2f} (should be 0)\")\n",
    "\tprint(f\" - tokensConsumed: {averages['tokensConsumed']:.2f} (should be around 1 - 2 tokens per request)\")\n",
    "\tprint(f\" - processingTimeInMs: {averages['processingTimeInMs']:.2f} (should be around 500)\")\n",
    "\tprint(f\" - requestsPerMinute: {averages['requestsPerMinute']:.2f} (should be around 2 - 4 requests per minute)\")\n",
    "\n",
    "summary_seconds_total = 0\n",
    "summary_days_total = 0\n",
    "summary_count_total = 0\n",
    "summary_success_total = 0\n",
    "\n",
    "# Summary for all domains (entire process)\n",
    "for domain in domains:\n",
    "\tdomain_name = domains_map[domain]\n",
    "\tdomain_name = f\"amazon.{domain_name}\"\n",
    "\tprint(f\"Domain '{domain_name}' (domain_id = '{domain}') summary:\")\n",
    "\tproducts_total = len(domains[domain][\"json_files\"]) # type: ignore\n",
    "\tproducts_failed = len(domains[domain][\"failed_files\"]) # type: ignore\n",
    "\tproducts_success_count = products_total - products_failed\n",
    "\tsuccess_rate = products_success_count / products_total * 100 # type: ignore\n",
    "\tprint(f\" - fetched data for {products_success_count}/{products_total} products ({success_rate:.3f} %)\") # type: ignore\n",
    "\terrors = domains[domain][\"errors_dict_sorted\"] # type: ignore\n",
    "\terrors_unique = domains[domain][\"errors_dict_sorted_counts\"] # type: ignore\n",
    "\tmost_common_error = list(errors_unique.keys())[0] # type: ignore\n",
    "\tprint(f\" - {products_failed} invalid results ({100 - success_rate:.3f} %) with {len(errors_unique)} unique errors\") # type: ignore\n",
    "\tprint(f\" - most common error: '{most_common_error[:50]}...' with {errors_unique[most_common_error]} products\") # type: ignore\n",
    "\tprint(f\"'{domain_name}' averages:\")\n",
    "\tpretty_print_averages(domains[domain][\"averages\"]) # type: ignore\n",
    "\tseconds_total = domains[domain][\"seconds_total\"] # type: ignore\n",
    "\tdays_total = domains[domain][\"days_total\"] # type: ignore\n",
    "\tsummary_seconds_total += seconds_total # type: ignore\n",
    "\tsummary_days_total += days_total # type: ignore\n",
    "\tsummary_count_total += products_total\n",
    "\tsummary_success_total += products_success_count\n",
    "\tprint(f\" - took {seconds_total:.2f} seconds ({days_total:.2f} days) to fetch data for {products_total} products\") # type: ignore\n",
    "\tprint(\"\")\n",
    "print(\"\")\n",
    "print(f\"Managed to fetch data for {summary_success_total}/{summary_count_total} products ({summary_success_total / summary_count_total * 100:.3f} %)\") # type: ignore\n",
    "print(\"\")\n",
    "print(f\"The entire process took {summary_seconds_total:.2f} seconds ({summary_days_total:.2f} days) non-stop fetching\") # type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple calculation of time needed to fetch all products\n",
    "tokens_per_request = 1.5\n",
    "refill_rate = 5\n",
    "requests_per_minute = refill_rate / tokens_per_request\n",
    "requests_per_day = requests_per_minute * 60 * 24\n",
    "print(f\"Requests per minute: {requests_per_minute}\")\n",
    "print(f\"Requests per day: {requests_per_day}\")\n",
    "files_count = len(domains[\"1\"][\"json_files\"]) # type: ignore\n",
    "time_to_fetch_all_products = files_count / requests_per_day\n",
    "print(f\"Time to fetch all products for a single domain ({files_count}): {time_to_fetch_all_products} days\")\n",
    "time_to_fetch_all_domains = time_to_fetch_all_products * len(domains)\n",
    "print(f\"Time to fetch products for all domains ({len(domains)}): {time_to_fetch_all_domains} days\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
