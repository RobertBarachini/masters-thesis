{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from re import sub\n",
    "\n",
    "# Project imports\n",
    "sys.path.append(os.getcwd())\n",
    "# Finance utils\n",
    "from src.py.analysis.yahoo.stocks.finance_df_utils import load_stock_csv, df_add_data, get_figure, filter_df_by_date, add_vline_annotation, save_fig, get_safe_filename, get_grouped_df\n",
    "# Events\n",
    "from src.py.analysis.events import events\n",
    "# CPI\n",
    "import importlib\n",
    "sys.path.append(os.getcwd())\n",
    "cpi_adjust = importlib.import_module(\"src.py.scraping.world-bank.cpi_adjust\")\n",
    "cpi_adjust.initialize_cpi(date_cutoff=\"2018-08-01\", jagged=False) # jagged=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_stocks_csv_root = \"data/scraped/yahoo/stocks/csv\"\n",
    "path_companies_index = \"data/scraped/yahoo/sectors/index_stocks.json\"\n",
    "path_output_root = \"data/analysis/yahoo/stocks\"\n",
    "\n",
    "if os.path.exists(path_output_root) is False:\n",
    "\tos.makedirs(path_output_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df_msft = load_stock_csv(os.path.join(path_stocks_csv_root, \"MSFT.csv\"))\n",
    "\n",
    "# Filter after 2018-10-01 (leave some for SMA)\n",
    "df_msft = filter_df_by_date(df_msft, start_date=\"2018-10-01\")\n",
    "\n",
    "# Adjust for inflation\n",
    "df_msft = cpi_adjust.adjust_for_inflation(df_msft, \"USA\", columns=[\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\"])\n",
    "\n",
    "# Add data\n",
    "df_msft = df_add_data(df_msft)\n",
    "\n",
    "# Filter after 2019-01-01\n",
    "df_msft = filter_df_by_date(df_msft, start_date=\"2019-01-01\")\n",
    "\n",
    "# Resample to weekly\n",
    "df_msft = df_msft.resample('W').mean()\n",
    "\n",
    "# Show tail\n",
    "df_msft.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative using Volume_value as Volume (abs(Open+Close) / 2 * Volume)\n",
    "\n",
    "# Load data\n",
    "df_msft = load_stock_csv(os.path.join(path_stocks_csv_root, \"MSFT.csv\"))\n",
    "\n",
    "# Filter after 2018-10-01 (leave some for SMA)\n",
    "df_msft = filter_df_by_date(df_msft, start_date=\"2018-10-01\")\n",
    "\n",
    "# Add Volume_value column\n",
    "df_msft[\"Volume_value\"] = (abs(df_msft[\"Open\"] + df_msft[\"Close\"]) / 2) * df_msft[\"Volume\"]\n",
    "# Replace Volume with Volume_value and drop Volume_value\n",
    "df_msft[\"Volume\"] = df_msft[\"Volume_value\"]\n",
    "df_msft = df_msft.drop(columns=[\"Volume_value\"])\n",
    "\n",
    "# Adjust for inflation - now with Volume\n",
    "df_msft = cpi_adjust.adjust_for_inflation(df_msft, \"USA\", columns=[\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\"])\n",
    "\n",
    "# Add data\n",
    "df_msft = df_add_data(df_msft)\n",
    "\n",
    "# Filter after 2019-01-01\n",
    "df_msft = filter_df_by_date(df_msft, start_date=\"2019-01-01\")\n",
    "\n",
    "# Resample to weekly\n",
    "df_msft = df_msft.resample('W').mean()\n",
    "\n",
    "# Show tail\n",
    "df_msft.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample:\n",
    "\n",
    "options = {\n",
    " \"w\": 1280,\n",
    " \"h\": 720,\n",
    " # \"traces\": [\"candlestick\"],\n",
    " \"traces\": [],\n",
    " \"margin\": {\"r\": 0, \"t\": 60, \"b\": 0, \"l\": 0},\n",
    " \"labels\": [\"Price ($)\", \"Volume\", \"MACD\"],\n",
    "}\n",
    "\n",
    "# Get figure\n",
    "fig = get_figure(df_msft, \"Microsoft (MSFT) stock price\", options)\n",
    "\n",
    "# # Add annotation\n",
    "# event_covid_crash = {\n",
    "#   \"date\": datetime(2020, 2, 20),\n",
    "#   # \"annotation\": \"COVID-19 market crash\"\n",
    "# \t\"annotation\": \"MC\"\n",
    "# }\n",
    "# add_vline_annotation(fig, event_covid_crash)\n",
    "\n",
    "# # Add annotation\n",
    "# event_covid_crash_end = {\n",
    "#   \"date\": datetime(2020, 4, 7),\n",
    "#   \"annotation\": \"MC end\"\n",
    "# }\n",
    "# add_vline_annotation(fig, event_covid_crash_end)\n",
    "\n",
    "# for event in events:\n",
    "# \tadd_vline_annotation(fig, event)\n",
    "\n",
    "\n",
    "# Show figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load companies\n",
    "companies = json.load(open(path_companies_index, \"r\"))\n",
    "companies = {company[\"symbol\"]: company for company in companies}\n",
    "print(f\"Loaded index with {len(companies)} companies.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_index_values(df: pd.DataFrame, date: str) -> pd.DataFrame:\n",
    "\tindex_value = df.loc[date, \"Open\"]\n",
    "\tdf[\"Open_I\"] = df[\"Open\"] / index_value * 100  # type: ignore\n",
    "\tindex_value = df.loc[date, \"High\"]\n",
    "\tdf[\"High_I\"] = df[\"High\"] / index_value * 100  # type: ignore\n",
    "\tindex_value = df.loc[date, \"Low\"]\n",
    "\tdf[\"Low_I\"] = df[\"Low\"] / index_value * 100  # type: ignore\n",
    "\tindex_value = df.loc[date, \"Close\"]\n",
    "\tdf[\"Close_I\"] = df[\"Close\"] / index_value * 100  # type: ignore\n",
    "\tindex_value = df.loc[date, \"Adj Close\"]\n",
    "\tdf[\"Adj_Close_I\"] = df[\"Adj Close\"] / index_value * 100  # type: ignore\n",
    "\t# index_value = df.loc[date, \"Volume\"]\n",
    "\t# df[\"Volume_I\"] = df[\"Volume\"] / index_value * 100  # type: ignore\n",
    "\t# NOTE: this approach can scramble the the values because of relative scaling\n",
    "\t# \t\t  as a consequence, the values like High_I can be lower than Low_I\n",
    "\t# Invert the values where High < Low\n",
    "\t# df[\"temp\"] = df[\"High_I\"]\n",
    "\t# df.loc[df[\"High_I\"] < df[\"Low_I\"],\n",
    "\t#        \"High_I\"] = df.loc[df[\"High_I\"] < df[\"Low_I\"], \"Low_I\"]\n",
    "\t# df.loc[df[\"High_I\"] < df[\"Low_I\"],\n",
    "\t#        \"Low_I\"] = df.loc[df[\"High_I\"] < df[\"Low_I\"], \"temp\"]\n",
    "\t# # Switch the Close_I, Open_I for all rows where their order is different from Close, Open\n",
    "\t# rows_og = df[\"Close\"] < df[\"Open\"]\n",
    "\t# rows_i = df[\"Close_I\"] < df[\"Open_I\"]\n",
    "\t# mask = rows_og != rows_i\n",
    "\t# count_pre = mask.sum()\n",
    "\t# df.loc[mask, \"temp\"] = df.loc[mask, \"Close_I\"]\n",
    "\t# df.loc[mask, \"Close_I\"] = df.loc[mask, \"Open_I\"]\n",
    "\t# df.loc[mask, \"Open_I\"] = df.loc[mask, \"temp\"]\n",
    "\t# rows_og = df[\"Close\"] < df[\"Open\"]\n",
    "\t# rows_i = df[\"Close_I\"] < df[\"Open_I\"]\n",
    "\t# mask = rows_og != rows_i\n",
    "\t# count_post = mask.sum()\n",
    "\t# # Drop temp column\n",
    "\t# df.drop(columns=[\"temp\"], inplace=True)\n",
    "\t# NOTE: a more complete approach would be to scale Open_I, High_I, and Low_I proportionally to Open, High, and Low using Close_I as a base\n",
    "\t#       calculated from the relative scaling of Close_I and Close - each daily measurement / candle should retain the same proportions and order\n",
    "\treturn df\n",
    "\n",
    "# Replace column values of Open with Open_I, High with High_I, Low with Low_I, Close with Close_I, Adj Close with Adj_Close_I, Volume with Volume_I\n",
    "def replace_by_index_value(df: pd.DataFrame) -> pd.DataFrame:\n",
    "\tif \"Open_I\" in df.columns:\n",
    "\t\t# drop original columns\n",
    "\t\tdf = df.drop(\n",
    "\t\t    columns=[\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\"])#, \"Volume\"])\n",
    "\t\t# rename columns\n",
    "\t\tdf = df.rename(\n",
    "\t\t    columns={\n",
    "\t\t        \"Open_I\": \"Open\",\n",
    "\t\t        \"High_I\": \"High\",\n",
    "\t\t        \"Low_I\": \"Low\",\n",
    "\t\t        \"Close_I\": \"Close\",\n",
    "\t\t        \"Adj_Close_I\": \"Adj Close\",\n",
    "\t\t        # \"Volume_I\": \"Volume\"\n",
    "\t\t    })\n",
    "\t# put the columns in the original order\n",
    "\tdf = df[[\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\"]]\n",
    "\treturn df\n",
    "\n",
    "# Load data\n",
    "df_msft = load_stock_csv(os.path.join(path_stocks_csv_root, \"MSFT.csv\"))\n",
    "# Filter after 2018-10-01 (leave some for SMA)\n",
    "df_msft = filter_df_by_date(df_msft, start_date=\"2018-08-01\")\n",
    "# Adjust for inflation\n",
    "df_msft = cpi_adjust.adjust_for_inflation(df_msft, \"USA\", columns=[\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\"])\n",
    "# Filter after 2019-01-01\n",
    "df_msft = filter_df_by_date(df_msft, start_date=\"2019-01-01\")\n",
    "# Add index values\n",
    "df_msft = add_index_values(df_msft, \"2019-01-02\")\n",
    "# Replace by index value\n",
    "df_msft = replace_by_index_value(df_msft)\n",
    "# df_scaled.head()\n",
    "# show df from 2019-01-01 to 2019-01-05\n",
    "df_msft.loc[\"2019-01-01\":\"2019-01-05\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NOTE: ORIGINAL\n",
    "\n",
    "# # Load all CSVs\n",
    "# dfs_dict = {}\n",
    "# dfs_fails = {}\n",
    "# print(f\"Loading {len(companies)} company stocks CSVs.\")\n",
    "# print(\"\")\n",
    "# for i, (ticker, company) in enumerate(companies.items()):\n",
    "# \tprint(f\"{i+1}/{len(companies)} [{ticker}]     \", end=\"\\r\")\n",
    "# \ttry:\n",
    "# \t\t# Load CSV\n",
    "# \t\tdf = load_stock_csv(os.path.join(path_stocks_csv_root, f\"{ticker}.csv\"))\n",
    "# \t\t# Check if data is within range (first date < 2018-10-01)\n",
    "# \t\tif df.index[0] > datetime(2018, 8, 1):\n",
    "# \t\t\traise Exception(\"Data starts after 2018-08-01.\")\n",
    "# \t\t# Check if data is within range (last date > 2023-12-01)\n",
    "# \t\tif df.index[-1] < datetime(2023, 12, 1):\n",
    "# \t\t\traise Exception(\"Data ends before 2023-12-01.\")\n",
    "# \t\t# Plotting interval is 2019-01-01 to 2023-12-26\n",
    "# \t\t# start_date is set to 2018-10-01 to leave some for window\n",
    "# \t\tdf = filter_df_by_date(df, start_date=\"2018-08-01\", end_date=\"2023-12-26\")\n",
    "# \t\t# Adjust for CPI\n",
    "# \t\tdf = cpi_adjust.adjust_for_inflation(df, \"USA\", columns=[\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\"])\n",
    "# \t\t# Remove all rows with NaN values\n",
    "# \t\tdf = df.dropna()\n",
    "# \t\t# Add index values\n",
    "# \t\tdf = add_index_values(df, \"2019-01-02\") # 2019-01-01 is a holiday (no trading)\n",
    "# \t\t# # Replace by index value\n",
    "# \t\tdf = replace_by_index_value(df)\n",
    "# \t\t# Add to dict\n",
    "# \t\tdfs_dict[ticker] = df\n",
    "# \texcept Exception as e:\n",
    "# \t\tdfs_fails[ticker] = e\n",
    "# print(\"\")\n",
    "# print(f\"Loaded {len(dfs_dict)} company stocks CSVs.\")\n",
    "# print(f\"{len(dfs_fails)} failed to load or do not meet the criteria.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: ADJUSTED - using Volume_value\n",
    "\n",
    "# Load all CSVs\n",
    "dfs_dict = {}\n",
    "dfs_fails = {}\n",
    "print(f\"Loading {len(companies)} company stocks CSVs.\")\n",
    "print(\"\")\n",
    "for i, (ticker, company) in enumerate(companies.items()):\n",
    "\tprint(f\"{i+1}/{len(companies)} [{ticker}]     \", end=\"\\r\")\n",
    "\ttry:\n",
    "\t\t# Load CSV\n",
    "\t\tdf = load_stock_csv(os.path.join(path_stocks_csv_root, f\"{ticker}.csv\"))\n",
    "\t\t# Check if data is within range (first date < 2018-10-01)\n",
    "\t\tif df.index[0] > datetime(2018, 8, 1):\n",
    "\t\t\traise Exception(\"Data starts after 2018-08-01.\")\n",
    "\t\t# Check if data is within range (last date > 2023-12-01)\n",
    "\t\tif df.index[-1] < datetime(2023, 12, 1):\n",
    "\t\t\traise Exception(\"Data ends before 2023-12-01.\")\n",
    "\t\t# Plotting interval is 2019-01-01 to 2023-12-26\n",
    "\t\t# start_date is set to 2018-10-01 to leave some for window\n",
    "\t\tdf = filter_df_by_date(df, start_date=\"2018-08-01\", end_date=\"2023-12-26\")\n",
    "\t\t# Add Volume_value column\n",
    "\t\tdf[\"Volume_value\"] = (abs(df[\"Open\"] + df[\"Close\"]) / 2) * df[\"Volume\"]\n",
    "\t\t# Replace Volume with Volume_value and drop Volume_value\n",
    "\t\tdf[\"Volume\"] = df[\"Volume_value\"]\n",
    "\t\tdf = df.drop(columns=[\"Volume_value\"])\n",
    "\t\t# Adjust for CPI\n",
    "\t\tdf = cpi_adjust.adjust_for_inflation(df, \"USA\", columns=[\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\"])\n",
    "\t\t# Remove all rows with NaN values\n",
    "\t\tdf = df.dropna()\n",
    "\t\t# Add index values\n",
    "\t\tdf = add_index_values(df, \"2019-01-02\") # 2019-01-01 is a holiday (no trading)\n",
    "\t\t# # Replace by index value\n",
    "\t\tdf = replace_by_index_value(df)\n",
    "\t\t# Add to dict\n",
    "\t\tdfs_dict[ticker] = df\n",
    "\texcept Exception as e:\n",
    "\t\tdfs_fails[ticker] = e\n",
    "print(\"\")\n",
    "print(f\"Loaded {len(dfs_dict)} company stocks CSVs.\")\n",
    "print(f\"{len(dfs_fails)} failed to load or do not meet the criteria.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print fails grouped and sorted by error\n",
    "print(f\"Failed to load {len(dfs_fails)} company stocks CSVs.\")\n",
    "print(\"\")\n",
    "dfs_fails_grouped = { str(e): [] for e in set(dfs_fails.values()) }\n",
    "for ticker, e in dfs_fails.items():\n",
    "\tdfs_fails_grouped[str(e)].append(ticker)\n",
    "\n",
    "for e, tickers in sorted(dfs_fails_grouped.items(), key=lambda x: len(x[1]), reverse=True):\n",
    "\tprint(f\"{len(tickers)} companies: {e}\")\n",
    "\tprint(f\"{', '.join(tickers[:10])}...\")\n",
    "\tprint(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_msft = dfs_dict[\"MSFT\"]\n",
    "df_msft.loc[\"2019-01-01\":].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter companies (multiple filters)\n",
    "\n",
    "# Remove companies without profile\n",
    "companies = {k: v for k, v in companies.items() if \"profile\" in v}\n",
    "print(f\"{len(companies)} companies remaining after removing companies without profile.\")\n",
    "\n",
    "# Remove companies without sector (empty string should be removed)\n",
    "companies = {k: v for k, v in companies.items() if \"sector\" in v[\"profile\"] and v[\"profile\"][\"sector\"].strip() != \"\"}\n",
    "print(f\"{len(companies)} companies remaining after removing companies without sector.\")\n",
    "\n",
    "# Remove companies without industry (empty string should be removed)\n",
    "companies = {k: v for k, v in companies.items() if \"industry\" in v[\"profile\"] and v[\"profile\"][\"industry\"].strip() != \"\"}\n",
    "print(f\"{len(companies)} companies remaining after removing companies without industry.\")\n",
    "\n",
    "# Filter companies that are not in the dfs_dict\n",
    "for ticker in list(companies.keys()):\n",
    "\tif ticker not in dfs_dict:\n",
    "\t\tdel companies[ticker]\n",
    "# Filter dfs_dict that are not in the companies\n",
    "for ticker in list(dfs_dict.keys()):\n",
    "\tif ticker not in companies:\n",
    "\t\tdel dfs_dict[ticker]\n",
    "print(f\"{len(companies)} companies remaining after removing companies with data that doesn't fit criteria.\")\n",
    "print(f\"{len(dfs_dict)} (dfs_dict) == {len(companies)} (companies) = {len(dfs_dict) == len(companies)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sectors = {}\n",
    "industries = {}\n",
    "industries_to_sectors = {}\n",
    "for ticker, company in companies.items():\n",
    "\tsector = company[\"profile\"][\"sector\"]\n",
    "\tindustry = company[\"profile\"][\"industry\"]\n",
    "\tif sector not in sectors:\n",
    "\t\tsectors[sector] = []\n",
    "\tif industry not in industries:\n",
    "\t\tindustries[industry] = []\n",
    "\tif industry not in industries_to_sectors:\n",
    "\t\tindustries_to_sectors[industry] = sector\n",
    "\tsectors[sector].append(ticker)\n",
    "\tindustries[industry].append(ticker)\n",
    "\n",
    "print(f\"Fetched {len(sectors)} sectors and {len(industries)} industries from {len(companies)} companies.\")\n",
    "print(f\"Sample: Industry 'Software—Application' -> sector '{industries_to_sectors['Software—Application']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Sum all companies: {sum([len(tickers) for tickers in sectors.values()])}\")\n",
    "# Print sorted sectors by number of companies\n",
    "for sector, tickers in sorted(sectors.items(), key=lambda x: len(x[1]), reverse=True):\n",
    "\tprint(f\"- '{sector}': {len(tickers)} companies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print sorted industries by number of companies\n",
    "for industry, tickers in sorted(industries.items(), key=lambda x: len(x[1]), reverse=True):\n",
    "\tprint(f\"- '{industry}' (sector '{industries_to_sectors[industry]}'): {len(tickers)} companies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add columns Symbol, Sector, Industry to start of each dataframe and concatenate them into one big dataframe\n",
    "for symbol, df in dfs_dict.items():\n",
    "\tif \"Symbol\" in df.columns and df.columns.tolist()[0] == \"Symbol\": # ensure idempotence, manually reload if needed\n",
    "\t\tcontinue\n",
    "\tcompany = companies[symbol]\n",
    "\tdf[\"Symbol\"] = symbol\n",
    "\tdf[\"Sector\"] = company[\"profile\"][\"sector\"]\n",
    "\tdf[\"Industry\"] = company[\"profile\"][\"industry\"]\n",
    "\t# Put Symbol, Sector, Industry columns at the start\n",
    "\tcols = df.columns.tolist()\n",
    "\tcols = cols[-3:] + cols[:-3]\n",
    "\tdf = df[cols]\n",
    "\tdfs_dict[symbol] = df\n",
    "\n",
    "df_all = pd.concat(dfs_dict.values())\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print number of rows and columns\n",
    "print(f\"Number of rows: {df_all.shape[0]}\")\n",
    "print(f\"Number of columns: {df_all.shape[1]}\")\n",
    "\n",
    "# Print size in memory\n",
    "print(f\"Size in memory: {df_all.memory_usage().sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\n",
    " \t\"w\": 1280,\n",
    " \t\"h\": 720,\n",
    " # \"traces\": [\"candlestick\"],\n",
    " \t\"traces\": [],\n",
    "\t\"color_changes\": False,\n",
    " \t\"margin\": {\"r\": 0, \"t\": 60, \"b\": 0, \"l\": 0},\n",
    " \t\"labels\": [\"Index values\", \"Volume\", \"MACD\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dfs for all sectors\n",
    "dfs_sectors = {}\n",
    "for i, (sector, tickers) in enumerate(sectors.items()):\n",
    "\tprint(f\"{i+1}/{len(sectors)} Processing sector '{sector}' ({len(tickers)} companies)                                     \", end=\"\\r\")\n",
    "\tdf_sector = get_grouped_df(df_all, \"Sector\", sector, start_date=\"2019-01-01\")\n",
    "\t\n",
    "\t# Resample to weekly\n",
    "\t# df_sector = df_sector.resample('W').mean()\n",
    "\tvolume = df_sector[\"Volume\"].resample(\"W\").sum()\n",
    "\tvolume_sma = df_sector[\"SMA_volume\"].resample(\"W\").sum()\n",
    "\tdf_sector = df_sector.resample(\"W\").mean()\n",
    "\tdf_sector[\"Volume\"] = volume\n",
    "\tdf_sector[\"SMA_volume\"] = volume_sma\n",
    "\t\n",
    "\t# Add to dict\n",
    "\tdfs_sectors[sector] = df_sector\n",
    "\n",
    "df_sector_technology = dfs_sectors[\"Technology\"]\n",
    "# df_sector_technology.head(3)\n",
    "# df_sector_technology.tail(3)\n",
    "df_sector_technology.head(10)\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dfs for all industries\n",
    "dfs_industries = {}\n",
    "for i, (industry, tickers) in enumerate(industries.items()):\n",
    "\tprint(f\"{i+1}/{len(industries)} Processing industry '{industry}' from sector '{industries_to_sectors[industry]}' ({len(tickers)} companies)                                     \", end=\"\\r\")\n",
    "\tdf_industry = get_grouped_df(df_all, \"Industry\", industry, start_date=\"2019-01-01\")\n",
    "\t\n",
    "\t# Resample to weekly\n",
    "\t# df_industry = df_industry.resample('W').mean()\n",
    "\tvolume = df_industry[\"Volume\"].resample(\"W\").sum()\n",
    "\tvolume_sma = df_industry[\"SMA_volume\"].resample(\"W\").sum()\n",
    "\tdf_industry = df_industry.resample(\"W\").mean()\n",
    "\tdf_industry[\"Volume\"] = volume\n",
    "\tdf_industry[\"SMA_volume\"] = volume_sma\n",
    "\n",
    "\t# Add to dict\n",
    "\tdfs_industries[industry] = df_industry\n",
    "\n",
    "df_industry_software_application = dfs_industries[\"Software—Application\"]\n",
    "# df_industry_software_application.head(3)\n",
    "# df_industry_software_application.tail(3)\n",
    "df_industry_software_application.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate plots for all sectors\n",
    "for i, (sector, df_sector) in enumerate(dfs_sectors.items()):\n",
    "\tprint(f\"{i+1}/{len(sectors)} Generating plot for sector '{sector}' ({len(sectors[sector])} companies)                                     \", end=\"\\r\")\n",
    "\t# Get figure \n",
    "\tfig = get_figure(df_sector, f\"Market performance of {sector} sector ({len(sectors[sector])} companies)\", options=options)\n",
    "\t# Add annotation\n",
    "\tfor event in events:\n",
    "\t\tadd_vline_annotation(fig, event, textangle=-20)\n",
    "\t# Save figure\n",
    "\tsave_fig(fig, os.path.join(path_output_root, get_safe_filename(f\"sector-{sector}.png\")), scale=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate plots for all industries\n",
    "for i, (industry, df_industry) in enumerate(dfs_industries.items()):\n",
    "\tprint(f\"{i+1}/{len(industries)} Generating plot for industry '{industry}' from sector '{industries_to_sectors[industry]}' ({len(industries[industry])} companies)                                     \", end=\"\\r\")\n",
    "\t# Get figure\n",
    "\tfig = get_figure(df_industry, f\"Market performance of '{industry}' industry ({len(industries[industry])} companies; Sector '{industries_to_sectors[industry]}')\", options=options)\n",
    "\t# Add annotation\n",
    "\tfor event in events:\n",
    "\t\tadd_vline_annotation(fig, event, textangle=-20)\n",
    "\t# Save figure\n",
    "\tsave_fig(fig, os.path.join(path_output_root, get_safe_filename(f\"sector-{industries_to_sectors[industry]}-industry-{industry}.png\")), scale=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate df for df_all\n",
    "df_all_grouped = get_grouped_df(df_all, start_date=\"2019-01-01\")\n",
    "\n",
    "# Resample to weekly\n",
    "# df_all_grouped = df_all_grouped.resample('W').mean()\n",
    "volume = df_all_grouped[\"Volume\"].resample(\"W\").sum()\n",
    "volume_sma = df_all_grouped[\"SMA_volume\"].resample(\"W\").sum()\n",
    "df_all_grouped = df_all_grouped.resample(\"W\").mean()\n",
    "df_all_grouped[\"Volume\"] = volume\n",
    "df_all_grouped[\"SMA_volume\"] = volume_sma\n",
    "\n",
    "df_all_grouped.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options_alt = {\n",
    "\t\"w\": 1280,\n",
    "\t\"h\": 720,\n",
    "\t\"traces\": [],\n",
    "\t\"color_changes\": False,\n",
    "\t\"margin\": {\"r\": 0, \"t\": 60, \"b\": 0, \"l\": 0},\n",
    "\t\"labels\": [\"Index values\", \"Volume\", \"MACD\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get figure\n",
    "fig = get_figure(df_all_grouped, f\"Market performance (all {len(companies)} companies)\", options=options_alt)\n",
    "# Add annotations\n",
    "for event in events:\n",
    "\tadd_vline_annotation(fig, event)\n",
    "# Save figure\n",
    "save_fig(fig, os.path.join(path_output_root, get_safe_filename(f\"entire-market.png\")), scale=3)\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
