{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from plotly import graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from plotly import express as px\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "# Project imports\n",
    "sys.path.append(os.getcwd())\n",
    "# from src.py.utils.generic_utils import wrapper\n",
    "from src.py.scraping.keepa.keepa_analysis_utils import load_result_object, parse_csv, organize_csv, discretize_csv_smart, get_trends, get_timeseries_from_trends, remove_outliers, remove_outliers_csv\n",
    "\n",
    "# Finance utils\n",
    "from src.py.analysis.yahoo.stocks.finance_df_utils import add_vline_annotation\n",
    "# Events\n",
    "from src.py.analysis.events import events\n",
    "# CPI\n",
    "import importlib\n",
    "sys.path.append(os.getcwd())\n",
    "cpi_adjust = importlib.import_module(\"src.py.scraping.world-bank.cpi_adjust\")\n",
    "cpi_adjust.initialize_cpi(date_cutoff=\"2016-08-01\", jagged=False) # jagged=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Couldn't be bothered with releasing memory so you'll need to allocate like 12 GB of RAM to the VM for this to work (the last visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "path_category_index = \"data/keepa/generated/categories-domain-1.json\"\n",
    "path_output_root = \"data/keepa/generated/plots\"\n",
    "path_csv_root = \"data/keepa/generated/csv\"\n",
    "if not os.path.exists(path_output_root):\n",
    "\tos.makedirs(path_output_root)\n",
    "if not os.path.exists(path_csv_root):\n",
    "\tos.makedirs(path_csv_root)\n",
    "domain_name_map = { # for fig title\n",
    "\t\"1\": \"amazon.com\",\n",
    "\t\"2\": \"amazon.co.uk\",\n",
    "\t\"3\": \"amazon.de\",\n",
    "\t\"5\": \"amazon.co.jp\",\n",
    "}\n",
    "domain_country_map = { # for CPI - use domain id as filename\n",
    "\t\"1\": \"USA\",\n",
    "\t\"2\": \"GBR\",\n",
    "\t\"3\": \"DEU\",\n",
    "\t\"5\": \"JPN\",\n",
    "}\n",
    "\n",
    "# We use the same product index for all domains\n",
    "def get_index_categories(domain: str) -> dict:\n",
    "\t# Load index categories\n",
    "\tindex_categories = json.load(open(path_category_index))\n",
    "\t# Process category product paths (use domain 1 as base and replace with chosen domain)\n",
    "\tfor category in index_categories:\n",
    "\t\tfor asin, product in index_categories[category].items():\n",
    "\t\t\tproduct_path = product[\"file\"].replace(\"domains/1\", f\"domains/{domain}\")\n",
    "\t\t\tindex_categories[category][asin][\"file\"] = product_path\n",
    "\t\t\t# Add category to product object\n",
    "\t\t\tindex_categories[category][asin][\"category\"] = category\n",
    "\treturn index_categories\n",
    "\n",
    "def verify_products(index_categories: dict) -> dict:\n",
    "\t'''\n",
    "\t\tVerifies that all products in categories are unique and returns a dictionary of domain products.\n",
    "\t'''\n",
    "\tdomain_products = {} # asin -> product (simplified object)\n",
    "\tfor category in index_categories:\n",
    "\t\tfor asin, product in index_categories[category].items():\n",
    "\t\t\tif asin in domain_products:\n",
    "\t\t\t\tprint(f\"Duplicate product found: {asin}\")\n",
    "\t\t\t\traise Exception(f\"Duplicate product found: {asin}\")\n",
    "\t\t\telse:\n",
    "\t\t\t\tdomain_products[asin] = product\n",
    "\tprint(f\"Total unique products in domain: {len(domain_products)}\")\n",
    "\treturn domain_products\n",
    "\n",
    "def get_product(product_filepath: str) -> dict:\n",
    "\t'''\n",
    "\t\tLoads a product file and returns a product.\n",
    "\t'''\n",
    "\tresult_object = json.load(open(product_filepath))\n",
    "\tproduct = result_object[\"products\"][0]\n",
    "\tproduct[\"csv\"] = parse_csv(product[\"csv\"])\n",
    "\tproduct[\"csv\"] = organize_csv(product[\"csv\"])\n",
    "\tproduct[\"csv\"] = discretize_csv_smart(product[\"csv\"])\n",
    "\treturn product\n",
    "\n",
    "def load_products(category: str, index_categories: dict) -> dict:\n",
    "\t'''\n",
    "\t\tLoads all products in a category.\n",
    "\t'''\n",
    "\tproducts = {}\n",
    "\tfor i, (asin, product) in enumerate(index_categories[category].items()):\n",
    "\t\t# if i == 100: # debug purposes\n",
    "\t\t# \tbreak\n",
    "\t\tprint(f\"Loading product {i+1}/{len(index_categories[category])} ({asin})      \", end=\"\\r\")\n",
    "\t\ttry: # ensures that we can skip products that fail to load (e.g. due to missing files across domains)\n",
    "\t\t\tloaded_product = get_product(product[\"file\"])\n",
    "\t\t\tproducts[asin] = loaded_product\n",
    "\t\texcept Exception as e:\n",
    "\t\t\t# print(f\"Failed to load product {i+1}/{len(index_categories[category])} ({asin})\")\n",
    "\t\t\t# print(e)\n",
    "\t\t\tpass\n",
    "\tprint(\"\")\n",
    "\treturn products\n",
    "\n",
    "def csv_to_df(csv: dict, domain: str) -> pd.DataFrame:\n",
    "\t'''\n",
    "\t\tConverts a csv object to a pandas DataFrame.\n",
    "\t'''\n",
    "\tindividual_dfs = []\n",
    "\t# create a column \"time\" with the datetime values\n",
    "\tkept_keys = set([\"AMAZON\", \"NEW\", \"USED\", \"EBAY_NEW_SHIPPING\", \"EBAY_USED_SHIPPING\", \"COUNT_NEW\", \"COUNT_USED\"])#, \"COUNT_REVIEWS\"])\n",
    "\tfor timeseries_name in csv.keys():\n",
    "\t\tif timeseries_name not in kept_keys:\n",
    "\t\t\tcontinue\n",
    "\t\ttry:\n",
    "\t\t\tprices = csv[timeseries_name][0]\n",
    "\t\t\t# If it's not a count calculate the mean value and divide the dataframe by it\n",
    "\t\t\t# this is a substitute for calculating index near a date\n",
    "\t\t\tif \"count\" not in timeseries_name.lower():\n",
    "\t\t\t\tif len(prices) == 0:\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\tmean_value = np.mean(prices)\n",
    "\t\t\t\tif mean_value == 0 or np.isnan(mean_value):\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\tprices = prices / mean_value * 100\n",
    "\t\t\ttimes = csv[timeseries_name][1]\n",
    "\t\t\t\n",
    "\t\t\t# Remove outliers using stats.zscore\n",
    "\t\t\tthreshold = 2\n",
    "\t\t\tz = np.abs(stats.zscore(prices))\n",
    "\t\t\toutliers = z > threshold\n",
    "\t\t\toutliers_count = np.sum(outliers) # for debugging\n",
    "\t\t\t# Filter out the outliers from the prices and times\n",
    "\t\t\tprices = prices[~outliers]\n",
    "\t\t\ttimes = times[~outliers]\n",
    "\t\t\t# TODO: impute instead of remove?\n",
    "\n",
    "\t\t\t# Create a series and then a dataframe\n",
    "\t\t\tseries = pd.Series(prices, index=times, name=timeseries_name)\n",
    "\t\t\tdf = pd.DataFrame(series)\n",
    "\n",
    "\t\t\t# Insert rows for missing dates between 2016-08-01 and 2024-01-01\n",
    "\t\t\tdates = pd.date_range(start=\"2016-08-01\", end=\"2024-01-01\", freq=\"D\")\n",
    "\t\t\tdf = df.reindex(dates)\n",
    "\t\t\t# replace missing values with NaN\n",
    "\t\t\tdf.fillna(np.nan, inplace=True)\n",
    "\t\t\t# replace -1 with NaN\n",
    "\t\t\tdf.replace(-1, np.nan, inplace=True)\n",
    "\n",
    "\t\t\t# Adjust for CPI if not a count\n",
    "\t\t\tif \"count\" not in timeseries_name.lower():\n",
    "\t\t\t\tdf = cpi_adjust.adjust_for_inflation(df, domain_country_map[domain], columns=[timeseries_name])\n",
    "\n",
    "\t\t\t# Remove rows with entirely NaN values\n",
    "\t\t\tdf = df.dropna()\n",
    "\t\t\t\n",
    "\t\t\t# Add to individual dataframes list\n",
    "\t\t\tindividual_dfs.append(df)\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tpass\n",
    "\n",
    "\t# Concatenate all the individual dataframes on index\n",
    "\tproduct_df = pd.concat(individual_dfs, axis=1)\n",
    "\t\n",
    "\t# Fill missing values with NaN\n",
    "\tproduct_df.fillna(np.nan, inplace=True)\n",
    "\n",
    "\t# Cut everything before 2016-08-01\n",
    "\tproduct_df = product_df[\"2016-08-01\":]\n",
    "\treturn product_df\n",
    "\n",
    "def add_product_info_to_df(product_df: pd.DataFrame, product: dict) -> pd.DataFrame:\n",
    "\t'''\n",
    "\t\tAdds product information to the DataFrame.\n",
    "\t'''\n",
    "\tif \"asin\" in product_df.columns: # assume already added\n",
    "\t\treturn product_df\n",
    "\t# Add to front of DataFrame\n",
    "\tproduct_df.insert(0, \"asin\", product[\"asin\"])\n",
    "\tproduct_df.insert(1, \"category\", product[\"category\"])\n",
    "\treturn product_df\n",
    "\n",
    "# Merge all products into one DataFrame keeping all rows\n",
    "def merge_category_dfs(product_dfs: list[pd.DataFrame]) -> pd.DataFrame:\n",
    "\t'''\n",
    "\t\tMerges all product DataFrames into one DataFrame.\n",
    "\t'''\n",
    "\t# Change index for all dfs to a column named \"time\" as the first column and reset index\n",
    "\tfor product_df in product_dfs:\n",
    "\t\tproduct_df.insert(0, \"time\", product_df.index)\n",
    "\t\tproduct_df.reset_index(drop=True, inplace=True)\n",
    "\t# Merge all product dfs into one\n",
    "\tmerged_df = pd.concat(product_dfs, axis=0)\n",
    "\t# Reset index\n",
    "\tmerged_df.reset_index(drop=True, inplace=True)\n",
    "\treturn merged_df\n",
    "\n",
    "def get_category_product_dfs(category_name: str, index_categories: dict, domain_products: dict, domain: str) -> list[pd.DataFrame]:\n",
    "\t'''\n",
    "\t\tLoads all products in a category and returns them as DataFrames.\n",
    "\t'''\n",
    "\tproducts = load_products(category_name, index_categories)\n",
    "\tproduct_dfs = []\n",
    "\tfails = []\n",
    "\tfor i, product in enumerate(products.values()):\n",
    "\t\tprint(f\"Processing product {i+1}/{len(products)} ({product['asin']})      \", end=\"\\r\")\n",
    "\t\ttry:\n",
    "\t\t\tproduct_df = csv_to_df(product[\"csv\"], domain)\n",
    "\t\t\tproduct_df = add_product_info_to_df(product_df, domain_products[product[\"asin\"]])\n",
    "\t\t\tif len(product_df) == 0: # some products have no valid data\n",
    "\t\t\t\tfails.append(product[\"asin\"])\n",
    "\t\t\telse:\n",
    "\t\t\t\tproduct_dfs.append(product_df)\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tfails.append(product[\"asin\"])\n",
    "\tprint(\"\")\n",
    "\tprint(f\"Successfully loaded {len(product_dfs)}/{len(products)} product dfs\")\n",
    "\treturn product_dfs\n",
    "\n",
    "def get_category_product_dfs_merged(category_name: str, index_categories: dict, domain_products: dict, domain: str) -> pd.DataFrame:\n",
    "\t'''\n",
    "\t\tLoads all products in a category and returns them as a merged DataFrame.\n",
    "\t'''\n",
    "\tprint(f\"Loading product dfs for category '{category_name}'\")\n",
    "\tproduct_dfs = get_category_product_dfs(category_name, index_categories, domain_products, domain)\n",
    "\tprint(f\"Merging {len(product_dfs)} product dfs\")\n",
    "\tmerged_df = merge_category_dfs(product_dfs)\n",
    "\tprint(f\"Done! Merged {len(merged_df)} rows\")\n",
    "\treturn merged_df\n",
    "\n",
    "def get_aggregate_timeseries_df(merged_df: pd.DataFrame) -> pd.DataFrame:\n",
    "\t'''\n",
    "\t\tAggregates the merged DataFrame to get a DataFrame mean values for a specific column.\n",
    "\t'''\n",
    "\t# Create a copy\n",
    "\tmerged_df =\tmerged_df.copy()\n",
    "\n",
    "\n",
    "\t# Remove asin and category columns\n",
    "\tmerged_df = merged_df.drop(columns=[\"asin\", \"category\"])\n",
    "\n",
    "\t# if EBAY_NEW_SHIPPING and EBAY_USED_SHIPPING are not present, add them as NaN\n",
    "\tif \"EBAY_NEW_SHIPPING\" not in merged_df.columns:\n",
    "\t\tmerged_df[\"EBAY_NEW_SHIPPING\"] = np.nan\n",
    "\tif \"EBAY_USED_SHIPPING\" not in merged_df.columns:\n",
    "\t\tmerged_df[\"EBAY_USED_SHIPPING\"] = np.nan\n",
    "\t# if AMAZON is not present, add it as NaN\n",
    "\tif \"AMAZON\" not in merged_df.columns:\n",
    "\t\tmerged_df[\"AMAZON\"] = np.nan\n",
    "\n",
    "\t# Group by time and aggregate\n",
    "\t# NEW, USED, AMAZON, EBAY_NEW_SHIPPING, EBAY_USED_SHIPPING by mean\n",
    "\t# COUNT_NEW, COUNT_USED by sum\n",
    "\tdf_ts = merged_df.groupby(\"time\").agg({\"NEW\": \"mean\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"USED\": \"mean\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"AMAZON\": \"mean\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"EBAY_NEW_SHIPPING\": \"mean\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"EBAY_USED_SHIPPING\": \"mean\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"COUNT_NEW\": \"sum\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"COUNT_USED\": \"sum\"})\n",
    "\n",
    "\t# Linear interpolation for NaN values to eliminate any small gaps\n",
    "\tdf_ts.interpolate(method=\"time\", inplace=True)\n",
    "\t\n",
    "\t# Use np.convolve to get the rolling average for all columns\n",
    "\twindow = 30 * 3\n",
    "\tavg = np.ones(window) / window\n",
    "\tvalues_avg = np.convolve(df_ts[\"NEW\"].values, avg, mode=\"same\") # type: ignore\n",
    "\tdf_ts[\"NEW\"] = values_avg\n",
    "\tvalues_avg = np.convolve(df_ts[\"USED\"].values, avg, mode=\"same\") # type: ignore\n",
    "\tdf_ts[\"USED\"] = values_avg\n",
    "\tvalues_avg = np.convolve(df_ts[\"AMAZON\"].values, avg, mode=\"same\") # type: ignore\n",
    "\tdf_ts[\"AMAZON\"] = values_avg\n",
    "\tvalues_avg = np.convolve(df_ts[\"EBAY_NEW_SHIPPING\"].values, avg, mode=\"same\") # type: ignore\n",
    "\tdf_ts[\"EBAY_NEW_SHIPPING\"] = values_avg\n",
    "\tvalues_avg = np.convolve(df_ts[\"EBAY_USED_SHIPPING\"].values, avg, mode=\"same\") # type: ignore\n",
    "\tdf_ts[\"EBAY_USED_SHIPPING\"] = values_avg\n",
    "\t# values_avg = np.convolve(df_ts[\"COUNT_NEW\"].values, avg, mode=\"same\")\n",
    "\t# df_ts[\"COUNT_NEW\"] = values_avg\n",
    "\t# values_avg = np.convolve(df_ts[\"COUNT_USED\"].values, avg, mode=\"same\")\n",
    "\t# df_ts[\"COUNT_USED\"] = values_avg\n",
    "\n",
    "\t# Smooth again if not using rolling\n",
    "\twindow = 30\n",
    "\tavg = np.ones(window) / window\n",
    "\tvalues_avg = np.convolve(df_ts[\"NEW\"].values, avg, mode=\"same\") # type: ignore\n",
    "\tdf_ts[\"NEW\"] = values_avg\n",
    "\tvalues_avg = np.convolve(df_ts[\"USED\"].values, avg, mode=\"same\") # type: ignore\n",
    "\tdf_ts[\"USED\"] = values_avg\n",
    "\tvalues_avg = np.convolve(df_ts[\"AMAZON\"].values, avg, mode=\"same\") # type: ignore\n",
    "\tdf_ts[\"AMAZON\"] = values_avg\n",
    "\tvalues_avg = np.convolve(df_ts[\"EBAY_NEW_SHIPPING\"].values, avg, mode=\"same\") # type: ignore\n",
    "\tdf_ts[\"EBAY_NEW_SHIPPING\"] = values_avg\n",
    "\tvalues_avg = np.convolve(df_ts[\"EBAY_USED_SHIPPING\"].values, avg, mode=\"same\") # type: ignore\n",
    "\tdf_ts[\"EBAY_USED_SHIPPING\"] = values_avg\n",
    "\tvalues_avg = np.convolve(df_ts[\"COUNT_NEW\"].values, avg, mode=\"same\") # type: ignore\n",
    "\tdf_ts[\"COUNT_NEW\"] = values_avg\n",
    "\tvalues_avg = np.convolve(df_ts[\"COUNT_USED\"].values, avg, mode=\"same\") # type: ignore\n",
    "\tdf_ts[\"COUNT_USED\"] = values_avg\n",
    "\n",
    "\t# Add NaN for missing dates from 2016-08-01 to 2024-01-01\n",
    "\t# Some merged dfs start at 2020 and we want to preserve scale\n",
    "\tdates = pd.date_range(start=\"2016-08-01\", end=\"2024-01-01\", freq=\"D\")\n",
    "\tdf_ts = df_ts.reindex(dates)\n",
    "\n",
    "\t# Use linear interpolation to fill NaN values once again\n",
    "\tdf_ts.interpolate(method=\"time\", inplace=True)\n",
    "\n",
    "\t# Remove all rows before 2017-01-01\n",
    "\tdf_ts = df_ts[df_ts.index >= \"2017-01-01\"]\n",
    "\t# Remove all after 2023-10-01\n",
    "\tdf_ts = df_ts[df_ts.index <= \"2023-10-01\"]\n",
    "\treturn df_ts\n",
    "\n",
    "def get_fig(df: pd.DataFrame, title: str) -> go.Figure:\n",
    "\t'''\n",
    "\t\tReturns a plotly figure for a DataFrame.\n",
    "\t'''\n",
    "\t# First row are prices / indices, second row are counts\n",
    "\tfig = make_subplots(rows=2, cols=1, row_heights=[0.8, 0.2], shared_xaxes=True, vertical_spacing=0.02)\n",
    "\tcolors = px.colors.qualitative.Plotly\n",
    "\tfirst_row_columns = [\"NEW\", \"USED\", \"AMAZON\", \"EBAY_NEW_SHIPPING\", \"EBAY_USED_SHIPPING\"]\n",
    "\tsecond_row_columns = [\"COUNT_NEW\", \"COUNT_USED\"]\n",
    "\t\n",
    "\t# Add traces\n",
    "\tfor i, column in enumerate(first_row_columns):\n",
    "\t\tfig.add_trace(go.Scatter(x=df.index, y=df[column], mode=\"lines\", name=column, line=dict(color=colors[i])), row=1, col=1)\n",
    "\tdf_counts = df[second_row_columns]\n",
    "\tfor i, column in enumerate(second_row_columns):\n",
    "\t\tfig.add_trace(go.Bar(x=df_counts.index, y=df_counts[column], name=column, marker=dict(color=colors[i])), row=2, col=1)\n",
    "\t\n",
    "\t# Bower bar spacing\n",
    "\tfig.update_layout(bargap=0)\n",
    "\t# Remove bar borders\n",
    "\tfig.update_traces(marker_line_width=0, row=2, col=1)\n",
    "\t# Change barmode to stacked\n",
    "\tfig.update_layout(barmode=\"stack\")\n",
    "\n",
    "\t# Update layout\n",
    "\tfig.update_layout(title=title, xaxis_title=\"Time\", yaxis_title=\"Price\")\n",
    "\tfig.update_layout(width=1280, height=720)\n",
    "\tfig.update_layout(legend=dict(traceorder=\"normal\"))\n",
    "\n",
    "\t# Update axes\n",
    "\tfig.update_xaxes(showgrid=True)\n",
    "\tfig.update_xaxes(title_text=\"Time\", row=2, col=1)\n",
    "\tfig.update_yaxes(title_text=\"Price trends\", row=1, col=1)\n",
    "\tfig.update_yaxes(title_text=\"Count listings\", row=2, col=1)\n",
    "\t# Remove first row x-axis title\n",
    "\tfig.update_xaxes(title_text=\"\", row=1, col=1)\n",
    "\t\n",
    "\t# Add events\n",
    "\tfor event in events:\n",
    "\t\tfig = add_vline_annotation(fig, event, textangle=-20)\n",
    "\t\n",
    "\t# Set margins\n",
    "\tfig.update_layout(margin=dict(r=0, t=60, b=0, l=0))\n",
    "\treturn fig\n",
    "\n",
    "def process_domain(domain: str, can_overwrite: bool) -> dict:\n",
    "\t'''\n",
    "\t\tProcesses a domain.\n",
    "\n",
    "\t\tReturns a dictionary of merged DataFrames for each category with the category name as the key.\n",
    "\t'''\n",
    "\tmerged_dfs = {}\n",
    "\tprint(f\"Processing domain 'id={domain}'\")\n",
    "\t# Load index categories\n",
    "\tindex_categories = get_index_categories(domain)\n",
    "\tdomain_products = verify_products(index_categories)\n",
    "\t# Get merged df for each category\n",
    "\tfor i, category in enumerate(index_categories):\n",
    "\t\t# if category != \"VIDEO_CARD\": # debug purposes\n",
    "\t\t# \tcontinue\n",
    "\t\tfilepath = os.path.join(path_csv_root, f\"domain-{domain}-{category}.csv\")\n",
    "\t\tif os.path.exists(filepath) and can_overwrite == False:\n",
    "\t\t\tprint(f\"Loading existing file for category '{category}' ({i+1}/{len(index_categories)})\")\n",
    "\t\t\tmerged_df = pd.read_csv(filepath, index_col=0)\n",
    "\t\t\tmerged_df[\"time\"] = pd.to_datetime(merged_df[\"time\"])\n",
    "\t\t\tmerged_dfs[category] = merged_df\n",
    "\t\telse:\n",
    "\t\t\tprint(f\"Processing category '{category}' ({i+1}/{len(index_categories)})\")\n",
    "\t\t\tmerged_df = get_category_product_dfs_merged(category, index_categories, domain_products, domain)\n",
    "\t\t\tmerged_dfs[category] = merged_df\n",
    "\t\t\t# Save to csv\n",
    "\t\t\tmerged_df.to_csv(filepath)\n",
    "\tprint(f\"Done processing domain 'id={domain}'\")\n",
    "\tprint(\"\")\n",
    "\treturn merged_dfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get merged DataFrames for each category in each domain\n",
    "\n",
    "# Variables\n",
    "time_start = time.time()\n",
    "can_overwrite = False # True to refresh, False to use cached data\n",
    "domains_to_process = [\"1\", \"2\", \"3\", \"5\"]\n",
    "# domains_to_process = [\"3\"] # debug purposes\n",
    "merged_category_dfs_domains = {}\n",
    "\n",
    "# Process each domain to get merged DataFrames for each category\n",
    "for domain in domains_to_process:\n",
    "\tmerged_dfs = process_domain(domain, can_overwrite)\n",
    "\tmerged_category_dfs_domains[domain] = merged_dfs\n",
    "\n",
    "time_end = time.time()\n",
    "print(f\"Processed {len(domains_to_process)} domains in {time_end - time_start:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_row_counts(merged_category_dfs_domains: dict) -> pd.DataFrame:\n",
    "\t'''\n",
    "\t\tReturns a DataFrame with the row counts for each category in each domain.\n",
    "\t'''\n",
    "\t# set df with columns as domains and rows as categories (category names as index)\n",
    "\tdf = pd.DataFrame()\n",
    "\tsorted_category_names = sorted(merged_category_dfs_domains[list(merged_category_dfs_domains.keys())[0]].keys())\n",
    "\tfor domain, merged_category_dfs in merged_category_dfs_domains.items():\n",
    "\t\tfor category in sorted_category_names:\n",
    "\t\t\tmerged_df = merged_category_dfs[category]\n",
    "\t\t\tdf.loc[category, domain_name_map[domain]] = len(merged_df)\n",
    "\t# add total row (sum of each column)\n",
    "\tdf.loc[\"Total\"] = df.sum()\n",
    "\t# convert to int\n",
    "\tdf = df.astype(int)\n",
    "\t# convert numbers to text, separated by commas for readability\n",
    "\tdf = df.applymap(lambda x: f\"{x:,}\") # type: ignore\n",
    "\t# add Category column to start and reset index\n",
    "\tdf = df.reset_index()\n",
    "\tdf.rename(columns={\"index\": \"Category\"}, inplace=True)\n",
    "\treturn df\n",
    "\n",
    "# def get_datapoint_counts(merged_category_dfs_domains: dict) -> pd.DataFrame:\n",
    "# \t'''\n",
    "# \t\tReturns a DataFrame with the row counts for each category in each domain.\n",
    "# \t'''\n",
    "# \t# set df with columns as domains and rows as categories (category names as index)\n",
    "# \tdf = pd.DataFrame()\n",
    "# \tsorted_category_names = sorted(merged_category_dfs_domains[list(merged_category_dfs_domains.keys())[0]].keys())\n",
    "# \tfor domain, merged_category_dfs in merged_category_dfs_domains.items():\n",
    "# \t\tfor category in sorted_category_names:\n",
    "# \t\t\tmerged_df = merged_category_dfs[category]\n",
    "# \t\t\t# df.loc[category, domain_name_map[domain]] = len(merged_df)\n",
    "# \t\t\t# get counts of non-Nan values for each column\n",
    "# \t\t\tcounts = merged_df.count()\n",
    "# \t\t\t# add to df\n",
    "# \t\t\tif \"NEW\" in counts.index:\n",
    "# \t\t\t\tdf.loc[category, f\"{domain_name_map[domain]} (new)\"] = counts[\"NEW\"]\n",
    "# \t\t\tif \"USED\" in counts.index:\n",
    "# \t\t\t\tdf.loc[category, f\"{domain_name_map[domain]} (used)\"] = counts[\"USED\"]\n",
    "# \t\t\tif \"AMAZON\" in counts.index:\n",
    "# \t\t\t\tdf.loc[category, f\"{domain_name_map[domain]} (Amazon)\"] = counts[\"AMAZON\"]\n",
    "# \t\t\tif \"EBAY_NEW_SHIPPING\" in counts.index:\n",
    "# \t\t\t\tdf.loc[category, f\"{domain_name_map[domain]} (eBay new)\"] = counts[\"EBAY_NEW_SHIPPING\"]\n",
    "# \t\t\tif \"EBAY_USED_SHIPPING\" in counts.index:\n",
    "# \t\t\t\tdf.loc[category, f\"{domain_name_map[domain]} (eBay used)\"] = counts[\"EBAY_USED_SHIPPING\"]\n",
    "# \t\t\tif \"COUNT_NEW\" in counts.index:\n",
    "# \t\t\t\tdf.loc[category, f\"{domain_name_map[domain]} (count new)\"] = counts[\"COUNT_NEW\"]\n",
    "# \t\t\tif \"COUNT_USED\" in counts.index:\n",
    "# \t\t\t\tdf.loc[category, f\"{domain_name_map[domain]} (count used)\"] = counts[\"COUNT_USED\"]\n",
    "# \t# add total row (sum of each column)\n",
    "# \tdf.loc[\"Total\"] = df.sum()\n",
    "# \t# Replace NaN with 0\n",
    "# \tdf.fillna(0, inplace=True)\n",
    "# \t# convert to int\n",
    "# \tdf = df.astype(int)\n",
    "# \t# convert numbers to text, separated by commas for readability\n",
    "# \tdf = df.applymap(lambda x: f\"{x:,}\") # type: ignore\n",
    "# \t# add Category column to start and reset index\n",
    "# \tdf = df.reset_index()\n",
    "# \tdf.rename(columns={\"index\": \"Category\"}, inplace=True)\n",
    "# \treturn df\n",
    "\n",
    "def get_products_counts(merged_category_dfs_domains: dict) -> pd.DataFrame:\n",
    "\t'''\n",
    "\t\tReturns a DataFrame with the row counts for each category in each domain.\n",
    "\t'''\n",
    "\t# set df with columns as domains and rows as categories (category names as index)\n",
    "\tdf = pd.DataFrame()\n",
    "\tsorted_category_names = sorted(merged_category_dfs_domains[list(merged_category_dfs_domains.keys())[0]].keys())\n",
    "\tfor domain, merged_category_dfs in merged_category_dfs_domains.items():\n",
    "\t\tfor category in sorted_category_names:\n",
    "\t\t\tmerged_df = merged_category_dfs[category]\n",
    "\t\t\t# get counts of non-Nan values for each column\n",
    "\t\t\tproducts = len(merged_df[\"asin\"].unique())\n",
    "\t\t\tdf.loc[category.replace(\"NETWORK_INTERFACE_CONTROLLER\", \"NIC\"), domain_name_map[domain]] = products\n",
    "\tdf.loc[\"Domain total\"] = df.sum()\n",
    "\t# Replace NaN with 0\n",
    "\tdf.fillna(0, inplace=True)\n",
    "\t# convert to int\n",
    "\tdf = df.astype(int)\n",
    "\t# convert numbers to text, separated by commas for readability\n",
    "\tdf = df.applymap(lambda x: f\"{x:,}\") # type: ignore\n",
    "\t# add Category column to start and reset index\n",
    "\tdf = df.reset_index()\n",
    "\tdf.rename(columns={\"index\": \"Category\"}, inplace=True)\n",
    "\treturn df\n",
    "\n",
    "def get_datapoint_counts(merged_category_dfs_domains: dict) -> pd.DataFrame:\n",
    "\t'''\n",
    "\t\tReturns a DataFrame with the row counts for each category in each domain.\n",
    "\t'''\n",
    "\t# set df with columns as domains and rows as categories (category names as index)\n",
    "\tdf = pd.DataFrame()\n",
    "\tsorted_category_names = sorted(merged_category_dfs_domains[list(merged_category_dfs_domains.keys())[0]].keys())\n",
    "\tfor domain, merged_category_dfs in merged_category_dfs_domains.items():\n",
    "\t\tfor category in sorted_category_names:\n",
    "\t\t\tmerged_df = merged_category_dfs[category]\n",
    "\t\t\t# get counts of non-Nan values for each column\n",
    "\t\t\tcounts = merged_df.count()\n",
    "\t\t\tproducts = len(merged_df[\"asin\"].unique())\n",
    "\t\t\tcounts_sum = 0\n",
    "\t\t\tvalid_columns = 0\n",
    "\t\t\tif \"NEW\" in counts.index:\n",
    "\t\t\t\tcounts_sum += counts[\"NEW\"]\n",
    "\t\t\t\tvalid_columns += 1\n",
    "\t\t\tif \"USED\" in counts.index:\n",
    "\t\t\t\tcounts_sum += counts[\"USED\"]\n",
    "\t\t\t\tvalid_columns += 1\n",
    "\t\t\tif \"AMAZON\" in counts.index:\n",
    "\t\t\t\tcounts_sum += counts[\"AMAZON\"]\n",
    "\t\t\t\tvalid_columns += 1\n",
    "\t\t\tif \"EBAY_NEW_SHIPPING\" in counts.index:\n",
    "\t\t\t\tcounts_sum += counts[\"EBAY_NEW_SHIPPING\"]\n",
    "\t\t\t\tvalid_columns += 1\n",
    "\t\t\tif \"EBAY_USED_SHIPPING\" in counts.index:\n",
    "\t\t\t\tcounts_sum += counts[\"EBAY_USED_SHIPPING\"]\n",
    "\t\t\t\tvalid_columns += 1\n",
    "\t\t\tif \"COUNT_NEW\" in counts.index:\n",
    "\t\t\t\tcounts_sum += counts[\"COUNT_NEW\"]\n",
    "\t\t\t\tvalid_columns += 1\n",
    "\t\t\tif \"COUNT_USED\" in counts.index:\n",
    "\t\t\t\tcounts_sum += counts[\"COUNT_USED\"]\n",
    "\t\t\t\tvalid_columns += 1\n",
    "\t\t\tdf.loc[category.replace(\"NETWORK_INTERFACE_CONTROLLER\", \"NIC\"), domain_name_map[domain]] = counts_sum\n",
    "\tdf.loc[\"Domain total\"] = df.sum()\n",
    "\t# Replace NaN with 0\n",
    "\tdf.fillna(0, inplace=True)\n",
    "\t# convert to int\n",
    "\tdf = df.astype(int)\n",
    "\t# convert numbers to text, separated by commas for readability\n",
    "\tdf = df.applymap(lambda x: f\"{x:,}\") # type: ignore\n",
    "\t# add Category column to start and reset index\n",
    "\tdf = df.reset_index()\n",
    "\tdf.rename(columns={\"index\": \"Category\"}, inplace=True)\n",
    "\treturn df\n",
    "\n",
    "def get_datapoint_percentage(merged_category_dfs_domains: dict) -> pd.DataFrame:\n",
    "\t'''\n",
    "\t\tReturns a DataFrame with the row counts for each category in each domain.\n",
    "\t'''\n",
    "\t# set df with columns as domains and rows as categories (category names as index)\n",
    "\tdf = pd.DataFrame()\n",
    "\tsorted_category_names = sorted(merged_category_dfs_domains[list(merged_category_dfs_domains.keys())[0]].keys())\n",
    "\tfor domain, merged_category_dfs in merged_category_dfs_domains.items():\n",
    "\t\tfor category in sorted_category_names:\n",
    "\t\t\tmerged_df = merged_category_dfs[category]\n",
    "\t\t\t# get counts of non-Nan values for each column\n",
    "\t\t\tcounts = merged_df.count()\n",
    "\t\t\tproducts = len(merged_df[\"asin\"].unique())\n",
    "\t\t\tcounts_sum = 0\n",
    "\t\t\tvalid_columns = 0\n",
    "\t\t\tif \"NEW\" in counts.index:\n",
    "\t\t\t\tcounts_sum += counts[\"NEW\"]\n",
    "\t\t\t\tvalid_columns += 1\n",
    "\t\t\tif \"USED\" in counts.index:\n",
    "\t\t\t\tcounts_sum += counts[\"USED\"]\n",
    "\t\t\t\tvalid_columns += 1\n",
    "\t\t\tif \"AMAZON\" in counts.index:\n",
    "\t\t\t\tcounts_sum += counts[\"AMAZON\"]\n",
    "\t\t\t\tvalid_columns += 1\n",
    "\t\t\tif \"EBAY_NEW_SHIPPING\" in counts.index:\n",
    "\t\t\t\tcounts_sum += counts[\"EBAY_NEW_SHIPPING\"]\n",
    "\t\t\t\tvalid_columns += 1\n",
    "\t\t\tif \"EBAY_USED_SHIPPING\" in counts.index:\n",
    "\t\t\t\tcounts_sum += counts[\"EBAY_USED_SHIPPING\"]\n",
    "\t\t\t\tvalid_columns += 1\n",
    "\t\t\tif \"COUNT_NEW\" in counts.index:\n",
    "\t\t\t\tcounts_sum += counts[\"COUNT_NEW\"]\n",
    "\t\t\t\tvalid_columns += 1\n",
    "\t\t\tif \"COUNT_USED\" in counts.index:\n",
    "\t\t\t\tcounts_sum += counts[\"COUNT_USED\"]\n",
    "\t\t\t\tvalid_columns += 1\n",
    "\t\t\tdays = 2710 # start=\"2016-08-01\", end=\"2024-01-01\"\n",
    "\t\t\t# each column can have at most n-days data points per product - calculate percentage of datapoints\n",
    "\t\t\tpercentage = counts_sum / (valid_columns * days * products) * 100\n",
    "\t\t\tdf.loc[category.replace(\"NETWORK_INTERFACE_CONTROLLER\", \"NIC\"), domain_name_map[domain]] = percentage\n",
    "\tdf.loc[\"Domain average\"] = df.mean()\n",
    "\t# convert numbers to text, separated by commas for readability\n",
    "\tdf = df.applymap(lambda x: f\"{round(x, 2)}%\") # type: ignore\n",
    "\t# add Category column to start and reset index\n",
    "\tdf = df.reset_index()\n",
    "\tdf.rename(columns={\"index\": \"Category\"}, inplace=True)\n",
    "\treturn df\n",
    "\n",
    "\n",
    "\n",
    "# Get row counts for each category in each domain\n",
    "# row_counts = get_row_counts(merged_category_dfs_domains)\n",
    "# row_counts\n",
    "products_counts = get_products_counts(merged_category_dfs_domains)\n",
    "# products_counts\n",
    "datapoint_counts = get_datapoint_counts(merged_category_dfs_domains)\n",
    "# datapoint_counts\n",
    "datapoint_percentage = get_datapoint_percentage(merged_category_dfs_domains)\n",
    "# datapoint_percentage\n",
    "\n",
    "# # Merge all three dataframes into one using columns to create multiindex data frame - subcolumns are \"P\" for product counts, \"D\" for data points, \"DP\" for data point percentage\n",
    "# # reindex\n",
    "# products_counts.set_index(\"Category\", inplace=True)\n",
    "# datapoint_counts.set_index(\"Category\", inplace=True)\n",
    "# datapoint_percentage.set_index(\"Category\", inplace=True)\n",
    "# merged_df = pd.concat([products_counts, datapoint_counts, datapoint_percentage], axis=1)\n",
    "# # rename columns\n",
    "# # merged_df.columns = pd.MultiIndex.from_tuples([(col, \"\") for col in merged_df.columns])\n",
    "# # rename columns - first could be \"P\" for product counts, \"D\" for data points, \"DP\" for data point percentage\n",
    "# merged_df.columns = pd.MultiIndex.from_tuples([(col, \"\") for col in merged_df.columns])\n",
    "# # sort columns\n",
    "# merged_df = merged_df.sort_index(axis=1)\n",
    "# # # reset index\n",
    "# merged_df = merged_df.reset_index()\n",
    "# # # rename columns\n",
    "# merged_df.rename(columns={\"\": \"Category\"}, inplace=True)\n",
    "# # # add total row\n",
    "# # merged_df.loc[\"Total\"] = merged_df.sum()\n",
    "# # rename top level index to \"Domain\"\n",
    "# merged_df.columns = merged_df.columns.set_names([\"Domain\", \"\"])\n",
    "# merged_df\n",
    "\n",
    "path_output = os.path.join(path_output_root, \"stats-products.csv\")\n",
    "products_counts.to_csv(path_output, index=False, sep=\"\\\\\")\n",
    "path_output = os.path.join(path_output_root, \"stats-datapoints.csv\")\n",
    "datapoint_counts.to_csv(path_output, index=False, sep=\"\\\\\")\n",
    "path_output = os.path.join(path_output_root, \"stats-datapoints-percentage.csv\")\n",
    "datapoint_percentage.to_csv(path_output, index=False, sep=\"\\\\\")\n",
    "\n",
    "# set category as index for all three\n",
    "products_counts.set_index(\"Category\", inplace=True)\n",
    "datapoint_counts.set_index(\"Category\", inplace=True)\n",
    "datapoint_percentage.set_index(\"Category\", inplace=True)\n",
    "# merge\n",
    "merged_counts = pd.concat([products_counts, datapoint_counts, datapoint_percentage], axis=1)\n",
    "# reset index\n",
    "merged_counts = merged_counts.reset_index()\n",
    "# replace NaN with /\n",
    "merged_counts.fillna(\"/\", inplace=True)\n",
    "# Join columns with the same names -\n",
    "# Save to csv\n",
    "path_output = os.path.join(path_output_root, \"stats.csv\")\n",
    "merged_counts.to_csv(path_output, index=False, sep=\"\\\\\")\n",
    "merged_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_video_card_1 = merged_category_dfs_domains[\"1\"][\"VIDEO_CARD\"]\n",
    "# df_video_card_1.head()\n",
    "# df_video_card_1.describe()\n",
    "# df_video_card_1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check memory of the total merged_category_dfs_domains\n",
    "total_memory_usage = 0\n",
    "for domain, merged_dfs in merged_category_dfs_domains.items():\n",
    "\tfor category, df in merged_dfs.items():\n",
    "\t\ttotal_memory_usage += df.memory_usage().sum()\n",
    "print(f\"Total memory usage: {total_memory_usage / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Explore single category\n",
    "# df = merged_category_dfs_domains[\"2\"][\"CHARGING_ADAPTER\"]\n",
    "# df = get_aggregate_timeseries_df(df)\n",
    "# # df.head()\n",
    "# fig = get_fig(df, \"Charging adapter trends\")\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate plots for each domain\n",
    "can_overwrite = False # True to regenerate plots, False to skip\n",
    "if os.path.exists(os.path.join(path_output_root, \"categories-per-domain\")) == False:\n",
    "\tos.makedirs(os.path.join(path_output_root, \"categories-per-domain\"))\n",
    "time_start = time.time()\n",
    "for i, (domain, merged_dfs) in enumerate(merged_category_dfs_domains.items()):\n",
    "\tfor j, (category, merged_df) in enumerate(merged_dfs.items()):\n",
    "\t\tpath_output = os.path.join(*[path_output_root, \"categories-per-domain\", f\"category-{category}-domain-{domain}.png\"])\n",
    "\t\tif os.path.exists(path_output) and can_overwrite == False:\n",
    "\t\t\tprint(f\"{i * len(merged_dfs) + j + 1}/{len(domains_to_process) * len(merged_dfs)} Skipping plot for category '{category}' for domain '{domain_name_map[domain]}' (id={domain})\")\n",
    "\t\t\tcontinue\n",
    "\t\tprint(f\"{i * len(merged_dfs) + j + 1}/{len(domains_to_process) * len(merged_dfs)} Generating plot for category '{category}' for domain '{domain_name_map[domain]}' (id={domain})\")\t\n",
    "\t\t# Get product count\n",
    "\t\tvalid_products_count = len(merged_df[\"asin\"].unique())\n",
    "\t\tprint(f\"- Valid products: {valid_products_count}\")\n",
    "\t\t# Get aggregate timeseries df\n",
    "\t\tprint(f\"- Generating aggregate timeseries...\")\n",
    "\t\tdf_ts = get_aggregate_timeseries_df(merged_df.copy()) # copy just in case\n",
    "\t\t# Plot\n",
    "\t\tprint(f\"- Generating plot...\")\n",
    "\t\ttitle = f\"Aggregate product price history trends for category '{category}' for domain '{domain_name_map[domain]}' ({valid_products_count} products)\"\n",
    "\t\tfig = get_fig(df_ts, title)\n",
    "\t\t# fig.show()\n",
    "\t\t# Save as png with 3x scale\n",
    "\t\tprint(f\"- Saving plot...\")\n",
    "\t\tfig.write_image(path_output, scale=3)\n",
    "\t\tprint(f\"- Saved plot to '{path_output}'\")\n",
    "print(\"\")\n",
    "time_end = time.time()\n",
    "print(f\"Generated plots for {len(domains_to_process)} domains in {time_end - time_start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generate plots for each category by merging all domains for that category into a single DataFrame\n",
    "\n",
    "# Merge all domains for each category\n",
    "if not os.path.exists(os.path.join(path_output_root, \"categories-all-domains\")):\n",
    "\tos.makedirs(os.path.join(path_output_root, \"categories-all-domains\"))\n",
    "selected_domains = domains_to_process\n",
    "selected_categories = list(merged_category_dfs_domains[\"1\"].keys())\n",
    "can_overwrite = False # True to refresh, False skip\n",
    "time_start = time.time()\n",
    "merged_category_dfs_categories = {}\n",
    "product_counts = {}\n",
    "print(f\"Selected domains: {selected_domains}\")\n",
    "print(f\"Selected categories: {selected_categories}\")\n",
    "print(f\"Merging all domains for each category\")\n",
    "for i, (domain, categories) in enumerate(merged_category_dfs_domains.items()):\n",
    "\tfor j, (category, df) in enumerate(categories.items()):\n",
    "\t\tif i == 0:\n",
    "\t\t\tmerged_category_dfs_categories[category] = df.copy()\n",
    "\t\t\tproduct_counts[category] = len(df[\"asin\"].unique())\n",
    "\t\telse:\n",
    "\t\t\t# just add rows to merged_category_dfs_categories[category]\n",
    "\t\t\tdf_to_add = df.copy()\n",
    "\t\t\tmerged_category_dfs_categories[category] = pd.concat([merged_category_dfs_categories[category], df_to_add], ignore_index=True)\n",
    "\t\t\tproduct_counts[category] += len(df[\"asin\"].unique())\n",
    "# Generate plots for each category\n",
    "for i, (category, df) in enumerate(merged_category_dfs_categories.items()):\n",
    "\tpath_output = os.path.join(*[path_output_root, \"categories-all-domains\", f\"category-{category}.png\"])\n",
    "\tif os.path.exists(path_output) and can_overwrite == False:\n",
    "\t\tprint(f\"{i+1}/{len(merged_category_dfs_categories)} Skipping plot for category '{category}' for all domains\")\n",
    "\t\tcontinue\n",
    "\tprint(f\"{i+1}/{len(merged_category_dfs_categories)} Generating plot for category '{category}' for all domains\")\n",
    "\t# Get product count - TODO: fix this to use counts from individual merged dfs\n",
    "\tvalid_products_count = product_counts[category]\n",
    "\tprint(f\"- Valid products: {valid_products_count}\")\n",
    "\t# Get aggregate timeseries df\n",
    "\tprint(f\"- Generating aggregate timeseries...\")\n",
    "\tdf_ts = get_aggregate_timeseries_df(df.copy()) # copy just in case\n",
    "\t# Plot\n",
    "\tprint(f\"- Generating plot...\")\n",
    "\ttitle = f\"Aggregate product price history trends for category '{category}' for all domains ({valid_products_count} products)\"\n",
    "\tfig = get_fig(df_ts, title)\n",
    "\t# fig.show()\n",
    "\t# Save as png with 3x scale\n",
    "\tprint(f\"- Saving plot...\")\n",
    "\tfig.write_image(path_output, scale=3)\n",
    "\tprint(f\"- Saved plot to '{path_output}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear some memory by deleting the merged_category_dfs_categories\n",
    "try:\n",
    "\tdel merged_category_dfs_categories\n",
    "except Exception as e:\n",
    "\tprint(\"Already deleted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generate plots for entire domain by merging all categories of a domain into a single DataFrame\n",
    "if not os.path.exists(os.path.join(path_output_root, \"domains-all-categories\")):\n",
    "\tos.makedirs(os.path.join(path_output_root, \"domains-all-categories\"))\n",
    "merged_domains = {}\n",
    "product_counts = {}\n",
    "can_overwrite = False # True to refresh, False to skip\n",
    "time_start = time.time()\n",
    "print(f\"Merging all categories for each domain\")\n",
    "for i, (domain, categories) in enumerate(merged_category_dfs_domains.items()):\n",
    "\tfor j, (category, df) in enumerate(categories.items()):\n",
    "\t\tif j == 0:\n",
    "\t\t\tmerged_domains[domain] = df.copy()\n",
    "\t\t\tproduct_counts[domain] = len(df[\"asin\"].unique())\n",
    "\t\telse:\n",
    "\t\t\t# just add rows to merged_domains[domain]\n",
    "\t\t\tdf_to_add = df.copy()\n",
    "\t\t\tmerged_domains[domain] = pd.concat([merged_domains[domain], df_to_add], ignore_index=True)\n",
    "\t\t\tproduct_counts[domain] += len(df[\"asin\"].unique())\n",
    "# Generate plots for each domain\n",
    "for i, (domain, df) in enumerate(merged_domains.items()):\n",
    "\tpath_output = os.path.join(*[path_output_root, \"domains-all-categories\", f\"domain-{domain}.png\"])\n",
    "\tif os.path.exists(path_output) and can_overwrite == False:\n",
    "\t\tprint(f\"{i+1}/{len(merged_domains)} Skipping plot for domain '{domain_name_map[domain]}'\")\n",
    "\t\tcontinue\n",
    "\tprint(f\"{i+1}/{len(merged_domains)} Generating plot for domain '{domain_name_map[domain]}'\")\n",
    "\t# Get product count - TODO: fix this to use counts from individual merged dfs\n",
    "\tvalid_products_count = product_counts[domain]\n",
    "\tprint(f\"- Valid products: {valid_products_count}\")\n",
    "\t# Get aggregate timeseries df\n",
    "\tprint(f\"- Generating aggregate timeseries...\")\n",
    "\tdf_ts = get_aggregate_timeseries_df(df.copy()) # copy just in case\n",
    "\t# Plot\n",
    "\tprint(f\"- Generating plot...\")\n",
    "\ttitle = f\"Aggregate product price history trends for all product categories for domain '{domain_name_map[domain]}' ({valid_products_count} products)\"\n",
    "\tfig = get_fig(df_ts, title)\n",
    "\t# fig.show()\n",
    "\t# Save as png with 3x scale\n",
    "\tprint(f\"- Saving plot...\")\n",
    "\tfig.write_image(path_output, scale=3)\n",
    "\tprint(f\"- Saved plot to '{path_output}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generate a plot for all categories and all domains - the big one\n",
    "df_big = merged_domains[\"1\"].copy()\n",
    "for domain, df in merged_domains.items():\n",
    "\tif domain == \"1\":\n",
    "\t\tcontinue\n",
    "\tdf_big = pd.concat([df_big, df], ignore_index=True)\n",
    "total_products_count = sum(product_counts.values())\n",
    "path_output = os.path.join(*[path_output_root, \"all-domains-products\", \"all-domains-products.png\"])\n",
    "# create folder if it doesn't exist\n",
    "if not os.path.exists(os.path.join(path_output_root, \"all-domains-products\")):\n",
    "\tos.makedirs(os.path.join(path_output_root, \"all-domains-products\"))\n",
    "# Generate plot\n",
    "print(f\"Generating plot for all domains and all categories\")\n",
    "# Get aggregate timeseries df\n",
    "print(f\"- Generating aggregate timeseries...\")\n",
    "df_ts = get_aggregate_timeseries_df(df_big)\n",
    "# Plot\n",
    "print(f\"- Generating plot...\")\n",
    "title = f\"Aggregate product price history trends for all product categories for all domains ({total_products_count} products)\"\n",
    "fig = get_fig(df_ts, title)\n",
    "# Save as png with 3x scale\n",
    "print(f\"- Saving plot...\")\n",
    "fig.write_image(path_output, scale=3)\n",
    "print(f\"- Saved plot to '{path_output}'\")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
