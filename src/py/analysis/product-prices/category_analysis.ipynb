{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from plotly import graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from plotly import express as px\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "# Project imports\n",
    "sys.path.append(os.getcwd())\n",
    "# from src.py.utils.generic_utils import wrapper\n",
    "from src.py.scraping.keepa.keepa_analysis_utils import load_result_object, parse_csv, organize_csv, discretize_csv_smart, get_trends, get_timeseries_from_trends, remove_outliers, remove_outliers_csv\n",
    "\n",
    "# Finance utils\n",
    "from src.py.analysis.yahoo.stocks.finance_df_utils import add_vline_annotation\n",
    "# Events\n",
    "from src.py.analysis.events import events\n",
    "# CPI\n",
    "import importlib\n",
    "sys.path.append(os.getcwd())\n",
    "cpi_adjust = importlib.import_module(\"src.py.scraping.world-bank.cpi_adjust\")\n",
    "cpi_adjust.initialize_cpi(date_cutoff=\"2016-08-01\", jagged=False) # jagged=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For prior analysis check src/py/scraping/keepa/category_analysis.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meaning of the columns\n",
    "# https://keepaapi.readthedocs.io/en/latest/api_methods.html\n",
    "# https://github.com/ukrexpo/keepa/blob/master/keepa.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_category_index = \"data/keepa/generated/categories-domain-1.json\"\n",
    "path_visualizations_root = \"data/keepa/generated/visualizations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = \"1\" # amazon.com\n",
    "domain_name_map = { # for fig title\n",
    "\t\"1\": \"amazon.com\",\n",
    "\t\"2\": \"amazon.co.uk\",\n",
    "\t\"3\": \"amazon.de\",\n",
    "\t\"5\": \"amazon.co.jp\",\n",
    "}\n",
    "domain_country_map = { # for CPI and filename\n",
    "\t\"1\": \"USA\",\n",
    "\t\"2\": \"GBR\",\n",
    "\t\"3\": \"DEU\",\n",
    "\t\"5\": \"JPN\",\n",
    "}\n",
    "print(f\"Working with domain_id '{domain}'\")\n",
    "print(f\"Working with domain '{domain_name_map[domain]}'\")\n",
    "print(f\"Working with domain country '{domain_country_map[domain]}'\")\n",
    "index_categories = json.load(open(path_category_index))\n",
    "print(f\"Loaded {len(index_categories)} categories:\")\n",
    "for category in index_categories:\n",
    "\tproduct_count = len(index_categories[category])\n",
    "\tprint(f\"- {category} ({product_count} products)\")\n",
    "\n",
    "# Process category product paths\n",
    "for category in index_categories:\n",
    "\tfor asin, product in index_categories[category].items():\n",
    "\t\tproduct_path = product[\"file\"].replace(\"domain/1\", f\"domain/{domain}\")\n",
    "\t\tindex_categories[category][asin][\"file\"] = product_path\n",
    "\t\t# Add category to product object\n",
    "\t\tindex_categories[category][asin][\"category\"] = category\n",
    "print(\"\")\n",
    "print(f\"Replaced 'domain/1' with 'domain/{domain}' in product paths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that all products in categories are unique\n",
    "category_products = {} # asin -> product (simplified object)\n",
    "for category in index_categories:\n",
    "\tfor asin, product in index_categories[category].items():\n",
    "\t\tif asin in category_products:\n",
    "\t\t\tprint(f\"Duplicate product found: {asin}\")\n",
    "\t\telse:\n",
    "\t\t\tcategory_products[asin] = product\n",
    "\n",
    "print(f\"Total unique products: {len(category_products)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_products_by_title(title):\n",
    "\tresults = []\n",
    "\tfor asin, product in category_products.items():\n",
    "\t\tif title.lower().replace(\" \", \"\") in product[\"title\"].lower().replace(\" \", \"\"):\n",
    "\t\t\tresults.append(asin)\n",
    "\treturn results\n",
    "\n",
    "query = \"ryzen 7 2700x\"\n",
    "sample_results = get_products_by_title(query)\n",
    "print(f\"Found {len(sample_results)} products matching '{query}':\")\n",
    "for asin in sample_results:\n",
    "\tprint(f\"- {category_products[asin]['title']} ({asin}) ; {category_products[asin]['category']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_product(product_filepath: str) -> dict:\n",
    "\t'''\n",
    "\t\tLoads a product file and returns a product.\n",
    "\t'''\n",
    "\tresult_object = json.load(open(product_filepath))\n",
    "\tproduct = result_object[\"products\"][0]\n",
    "\tproduct[\"csv\"] = parse_csv(product[\"csv\"])\n",
    "\tproduct[\"csv\"] = organize_csv(product[\"csv\"])\n",
    "\tproduct[\"csv\"] = discretize_csv_smart(product[\"csv\"])\n",
    "\treturn product\n",
    "\n",
    "sample_product_asin = \"B07B428M7F\"\n",
    "sample_product = get_product(category_products[sample_product_asin][\"file\"])\n",
    "# print(json.dumps(sample_product, indent=2, default=str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print timeseries names\n",
    "print(\"Timeseries names:\")\n",
    "for name in sample_product[\"csv\"].keys():\n",
    "\tprint(f\"- {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot sample product NEW price history\n",
    "fig = go.Figure()\n",
    "timeseries_name = \"NEW\" #\"COUNT_NEW\"\n",
    "prices = sample_product[\"csv\"][timeseries_name][0]\n",
    "times = sample_product[\"csv\"][timeseries_name][1]\n",
    "print(f\"Data points: {len(prices)}\")\n",
    "fig.add_trace(go.Scatter(x=times, y=prices, mode=\"lines+markers\", name=\"Price\"))\n",
    "fig.update_layout(title=f\"Product price 'NEW' of {sample_product['title']} ({sample_product['asin']})<br>Data points: {len(prices)}\", xaxis_title=\"Time\", yaxis_title=\"Price\")\n",
    "fig.update_xaxes(range=[\"2018-04-01\", \"2023-11-20\"])\n",
    "fig.update_layout(width=1200, height=500)\n",
    "fig.update_layout(margin=dict(l=0, r=10, t=60, b=0))\n",
    "path_output = os.path.join(path_visualizations_root, f\"sample-product-new-{sample_product['asin']}.png\")\n",
    "fig.write_image(path_output, scale=2)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sample product NEW - using z-score to remove outliers\n",
    "fig = go.Figure()\n",
    "import plotly.express as px\n",
    "# fig = make_subplots(rows=2, cols=1, shared_xaxes=True, vertical_spacing=0.02)\n",
    "timeseries_name = \"NEW\"\n",
    "prices = sample_product[\"csv\"][timeseries_name][0]\n",
    "times = sample_product[\"csv\"][timeseries_name][1]\n",
    "fig.add_trace(go.Scatter(x=times, y=prices, mode=\"lines+markers\", name=\"Price\", line=dict(color=px.colors.qualitative.Plotly[1])))\n",
    "threshold = 2 #1.8 # 2\n",
    "z = np.abs(stats.zscore(prices))\n",
    "outliers = z > threshold\n",
    "outliers_count = np.sum(outliers)\n",
    "# filter out the outliers from the prices and times\n",
    "prices = prices[~outliers]\n",
    "times = times[~outliers]\n",
    "print(f\"Data points: {len(prices)} (removed {outliers_count} outliers)\")\n",
    "fig.add_trace(go.Scatter(x=times, y=prices, mode=\"lines+markers\", name=f\"Price t={threshold}\", line=dict(color=px.colors.qualitative.Plotly[0])))\n",
    "fig.update_layout(title=f\"Product price 'NEW' of {sample_product['title']} ({sample_product['asin']})<br>Data points: {len(prices)} (removed {outliers_count} outliers)\", xaxis_title=\"Time\", yaxis_title=\"Price\")\n",
    "fig.update_xaxes(range=[\"2018-04-01\", \"2023-11-20\"])\n",
    "fig.update_layout(width=1200, height=500)\n",
    "fig.update_layout(margin=dict(l=0, r=10, t=60, b=0))\n",
    "path_output = os.path.join(path_visualizations_root, f\"sample-product-new-outliers-zscore-{str(threshold).replace('.', 'dot')}-{sample_product['asin']}.png\")\n",
    "fig.write_image(path_output, scale=2)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sample product NEW price history\n",
    "fig = go.Figure()\n",
    "timeseries_name = \"NEW\"\n",
    "csv_dev_3 = remove_outliers_csv(sample_product[\"csv\"], max_std_multiplier=2)\n",
    "prices = csv_dev_3[timeseries_name][0]\n",
    "times = csv_dev_3[timeseries_name][1]\n",
    "print(f\"Data points: {len(prices)} (removed {len(sample_product['csv'][timeseries_name][0]) - len(prices)} outliers)\")\n",
    "fig.add_trace(go.Scatter(x=times, y=prices, mode=\"lines\", name=\"Price\"))\n",
    "fig.update_layout(title=f\"Price history of {sample_product['title']} ({sample_product['asin']})\", xaxis_title=\"Time\", yaxis_title=\"Price\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "# DONE\n",
    "\n",
    "- change algo for outliers - moving average / moving std (use z-score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = \"COMPUTER_PROCESSOR\"\n",
    "\n",
    "def load_products(category: str, index_categories: dict) -> dict:\n",
    "\t'''\n",
    "\t\tLoads all products in a category.\n",
    "\t'''\n",
    "\tproducts = {}\n",
    "\tfor i, (asin, product) in enumerate(index_categories[category].items()):\n",
    "\t\t# if i == 100: # debug purposes\n",
    "\t\t# \tbreak\n",
    "\t\tprint(f\"Loading product {i+1}/{len(index_categories[category])} ({asin})      \", end=\"\\r\")\n",
    "\t\tproducts[asin] = get_product(product[\"file\"])\n",
    "\tprint(\"\")\n",
    "\treturn products\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "products = load_products(category, index_categories)\n",
    "print(f\"Loaded {len(products)} products in category '{category}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def csv_to_df(csv: dict) -> pd.DataFrame:\n",
    "# \t'''\n",
    "# \t\tConverts a csv object to a pandas DataFrame.\n",
    "# \t'''\n",
    "# \tindividual_dfs = []\n",
    "# \t# create a column \"time\" with the datetime values\n",
    "# \tfor timeseries_name in csv.keys():\n",
    "# \t\ttry:\n",
    "# \t\t\tprices = csv[timeseries_name][0]\n",
    "# \t\t\ttimes = csv[timeseries_name][1]\n",
    "# \t\t\tseries = pd.Series(prices, index=times, name=timeseries_name)\n",
    "# \t\t\tdf = pd.DataFrame(series)\n",
    "# \t\t\t# insert rows for missing dates\n",
    "# \t\t\tdf = df.resample(\"D\").asfreq()\n",
    "# \t\t\t# replace missing values with NaN\n",
    "# \t\t\tdf.fillna(np.nan, inplace=True)\n",
    "# \t\t\t# replace -1 with NaN\n",
    "# \t\t\tdf.replace(-1, np.nan, inplace=True)\n",
    "# \t\t\t# # resample to weekly frequency\n",
    "# \t\t\t# df = df.resample(\"W\").mean()\n",
    "# \t\t\t# fill NaN values by linear interpolation between the closest non-NaN values\n",
    "# \t\t\tdf.interpolate(method=\"time\", inplace=True)\n",
    "# \t\t\tthreshold = 3\n",
    "# \t\t\tdf = df[(np.abs(stats.zscore(df)) < threshold).all(axis=1)]\n",
    "# \t\t\t# smooth the data by taking the rolling mean with a window of 20\n",
    "# \t\t\tdf = df.rolling(window=20).mean()\n",
    "# \t\t\tindividual_dfs.append(df)\n",
    "# \t\texcept Exception as e:\n",
    "# \t\t\tpass\n",
    "# \t# concatenate all the individual dataframes on index\n",
    "# \t# product_df = pd.concat(individual_dfs, axis=1)\n",
    "# \t# product_df = pd.DataFrame(individual_dfs).transpose()\n",
    "# \tproduct_df = pd.concat(individual_dfs, axis=1)\n",
    "# \t# fill missing values with NaN\n",
    "# \tproduct_df.fillna(np.nan, inplace=True)\n",
    "# \t# fill NaN values by linear interpolation between the closest non-NaN values\n",
    "# \tproduct_df.interpolate(method=\"time\", inplace=True)\n",
    "# \t# # smooth the data by taking the rolling mean with a window of 20\n",
    "# \t# product_df = product_df.rolling(window=20).mean()\n",
    "# \treturn product_df\n",
    "\n",
    "\n",
    "def csv_to_df(csv: dict) -> pd.DataFrame:\n",
    "\t'''\n",
    "\t\tConverts a csv object to a pandas DataFrame.\n",
    "\t'''\n",
    "\tindividual_dfs = []\n",
    "\t# create a column \"time\" with the datetime values\n",
    "\tkept_keys = set([\"AMAZON\", \"NEW\", \"USED\", \"EBAY_NEW_SHIPPING\", \"EBAY_USED_SHIPPING\", \"COUNT_NEW\", \"COUNT_USED\"])#, \"COUNT_REVIEWS\"])\n",
    "\tfor timeseries_name in csv.keys():\n",
    "\t\tif timeseries_name not in kept_keys:\n",
    "\t\t\tcontinue\n",
    "\t\ttry:\n",
    "\t\t\tprices = csv[timeseries_name][0]\n",
    "\t\t\t#if True: #\"count\" not in timeseries_name.lower():\n",
    "\t\t\tif \"count\" not in timeseries_name.lower():\n",
    "\t\t\t\tif len(prices) == 0:\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\t# get mean value from prices numpy array\n",
    "\t\t\t\tmean_value = np.mean(prices)\n",
    "\t\t\t\t# divide the dataframe by the mean value and multiply by 100\n",
    "\t\t\t\tif mean_value == 0 or np.isnan(mean_value):\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\tprices = prices / mean_value * 100\n",
    "\t\t\ttimes = csv[timeseries_name][1]\n",
    "\t\t\t\n",
    "\t\t\t# Remove outliers using stats.zscore\n",
    "\t\t\tthreshold = 2\n",
    "\t\t\tz = np.abs(stats.zscore(prices))\n",
    "\t\t\toutliers = z > threshold\n",
    "\t\t\toutliers_count = np.sum(outliers)\n",
    "\t\t\t# filter out the outliers from the prices and times\n",
    "\t\t\tprices = prices[~outliers]\n",
    "\t\t\ttimes = times[~outliers]\n",
    "\t\t\t# TODO: impute instead of remove?\n",
    "\n",
    "\t\t\tseries = pd.Series(prices, index=times, name=timeseries_name)\n",
    "\t\t\tdf = pd.DataFrame(series)\n",
    "\t\t\t# insert rows for missing dates\n",
    "\t\t\t# df = df.resample(\"D\").asfreq()\n",
    "\t\t\t# insert rows for missing dates between 2019-01-01 and 2024-01-01\n",
    "\t\t\tdates = pd.date_range(start=\"2016-08-01\", end=\"2024-01-01\", freq=\"D\")\n",
    "\t\t\tdf = df.reindex(dates)\n",
    "\t\t\t# replace missing values with NaN\n",
    "\t\t\tdf.fillna(np.nan, inplace=True)\n",
    "\t\t\t# replace -1 with NaN\n",
    "\t\t\tdf.replace(-1, np.nan, inplace=True)\n",
    "\n",
    "\n",
    "\t\t\t# # fill NaN values by linear interpolation between the closest non-NaN values\n",
    "\t\t\t# df.interpolate(method=\"time\", inplace=True)\n",
    "\t\t\t# threshold = 3\n",
    "\t\t\t# df = df[(np.abs(stats.zscore(df)) < threshold).all(axis=1)]\n",
    "\t\t\t# # smooth the data by taking the rolling mean with a window of 20\n",
    "\t\t\t# # df = df.rolling(window=20).mean()\n",
    "\t\t\t# # # resample to weekly frequency\n",
    "\t\t\t# df = df.resample(\"M\").mean()\n",
    "\n",
    "\t\t\t# Resample to weekly frequency\n",
    "\t\t\tfrequency = \"W\"\n",
    "\t\t\t# this is incorrect as COUNT_NEW / COUNT_USED are the number of new/used offers, not the number of sales\n",
    "\t\t\t# if \"count\" in timeseries_name.lower():\n",
    "\t\t\t# \tdf = df.resample(frequency).sum()\n",
    "\t\t\t# else:\n",
    "\t\t\t# \tdf = df.resample(frequency).mean()\n",
    "\t\t\t# Correct:\n",
    "\t\t\t# df = df.resample(frequency).mean()\n",
    "\n",
    "\t\t\t# # Find the closes index to \"2019-01-01\" ; 2019-01-01 = 100\n",
    "\t\t\t# # and set corresponding value to 100 and divide all other values by this value\n",
    "\t\t\t# fixed_date_string = \"2019-01-01\"\n",
    "\t\t\t# fixed_date = pd.to_datetime(fixed_date_string)\n",
    "\t\t\t# fixed_date_timestamp = fixed_date.timestamp()\n",
    "\t\t\t# closest_index = pd.to_datetime(df.index).astype(int) / 10**9 - fixed_date_timestamp\n",
    "\t\t\t# closest_index = closest_index.to_numpy()\n",
    "\t\t\t# closest_index = np.abs(closest_index).argmin()\n",
    "\t\t\t# # find value at index\n",
    "\t\t\t# date_at_index = df.index[closest_index]\n",
    "\t\t\t# # get difference in days\n",
    "\t\t\t# days_difference = (date_at_index - fixed_date).days\n",
    "\t\t\t# # reject the product if the index is too far from the date\n",
    "\t\t\t# if days_difference > 7: # which number to set\n",
    "\t\t\t# \t# reject the product\n",
    "\t\t\t# \tcontinue\n",
    "\t\t\t# \t# raise Exception(f\"Index too far from {fixed_date_string}\")\n",
    "\t\t\t# value_at_index = df.iloc[closest_index].values[0]\n",
    "\t\t\t# # divide the dataframe by the value at the index and multiply by 100\n",
    "\t\t\t# df = df / value_at_index * 100\n",
    "\t\n",
    "\t\t\t# NOTE: moved up\n",
    "\t\t\t# # Best / most consistent so far\n",
    "\t\t\t# if \"count\" not in timeseries_name.lower():\n",
    "\t\t\t# \t# get mean value from prices numpy array\n",
    "\t\t\t# \tmean_value = np.mean(prices)\n",
    "\t\t\t# \t# divide the dataframe by the mean value and multiply by 100\n",
    "\t\t\t# \tdf = df / mean_value * 100\n",
    "\n",
    "\t\t\t# adjust for CPI\n",
    "\t\t\t# df = cpi_adjust.adjust_for_inflation(df, \"USA\", columns=[timeseries_name])\n",
    "\t\t\tif \"count\" not in timeseries_name.lower():\n",
    "\t\t\t\tdf = cpi_adjust.adjust_for_inflation(df, domain_country_map[domain], columns=[timeseries_name])\n",
    "\n",
    "\t\t\t# Remove NaN values\n",
    "\t\t\t# df = df.dropna()\n",
    "\t\t\t\n",
    "\t\t\t# add to individual dataframes list\n",
    "\t\t\tindividual_dfs.append(df)\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tpass\n",
    "\t# concatenate all the individual dataframes on index\n",
    "\tproduct_df = pd.concat(individual_dfs, axis=1)\n",
    "\t\n",
    "\t# fill missing values with NaN\n",
    "\tproduct_df.fillna(np.nan, inplace=True)\n",
    "\t\n",
    "\t# count reviews is problematic - needs to be resolved - negative reviews, swings by thousands per day, ...\n",
    "\t# omit for now\n",
    "\t# # count reviews is cumulative, so we need to convert to daily count\n",
    "\t# product_df[\"COUNT_REVIEWS\"] = product_df[\"COUNT_REVIEWS\"].diff()\n",
    "\t# # sometimes the values are negative - replace all negative values with NaN\n",
    "\t# product_df[\"COUNT_REVIEWS\"].clip(lower=0, inplace=True)\n",
    "\n",
    "\t# resample all but counts to weekly frequency with mean and the counts to weekly frquency with sum\n",
    "\n",
    "\n",
    "\t# cut everything before 2018-09-01\n",
    "\t# product_df = product_df[\"2018-09-01\":]\n",
    "\t# product_df = product_df[\"2017-01-01\":]\n",
    "\tproduct_df = product_df[\"2016-08-01\":]\n",
    "\treturn product_df\n",
    "\n",
    "product_df = csv_to_df(sample_product[\"csv\"])\n",
    "# product_df.head()\n",
    "# from 2019-01-01 display first 5 rows\n",
    "product_df[\"2019-01-01\":].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_percent_non_null(df: pd.DataFrame, timeseries_name: str) -> float:\n",
    "\t'''\n",
    "\t\tReturns the percentage of non-null values in a timeseries.\n",
    "\t'''\n",
    "\tnon_null_count = df[timeseries_name].count()\n",
    "\ttotal_count = df[timeseries_name].size\n",
    "\treturn non_null_count / total_count * 100\n",
    "\n",
    "timeseries_name = \"NEW\"\n",
    "print(f\"Percentage of non-null values in timeseries '{timeseries_name}': {get_percent_non_null(product_df, timeseries_name):.2f}%\")\n",
    "print(\"\")\n",
    "\n",
    "# print df info\n",
    "print(product_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df = product_df[\"2017-01-01\":]\n",
    "product_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all columns with lines (continous - no gaps)\n",
    "fig = go.Figure()\n",
    "for column in product_df.columns:\n",
    "\tfig.add_trace(go.Scatter(x=product_df.index, y=product_df[column], mode=\"lines+markers\", name=column))\n",
    "fig.update_layout(title=f\"Product history of {sample_product['title']} ({sample_product['asin']})\", xaxis_title=\"Time\", yaxis_title=\"Values\")\n",
    "fig.update_xaxes(range=[\"2018-04-01\", \"2023-11-20\"])\n",
    "fig.update_layout(width=1200, height=500)\n",
    "fig.update_layout(margin=dict(l=0, r=0, t=50, b=0))\n",
    "path_output = os.path.join(path_visualizations_root, f\"sample-product-history-{sample_product['asin']}.png\")\n",
    "fig.write_image(path_output, scale=2)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "- fix COUNT_REVIEWS (and similar columns) which can only increase over time (or at least not decrease) - for each value remove all later values which are smaller\n",
    "- add COUNT_REVIEWS_CHANGE - how much the value of COUNT_REVIEWS changed from the previous day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print product_df size in MB\n",
    "print(f\"Size of product_df: {product_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample to weekly\n",
    "def resample(product_df: pd.DataFrame, freq: str) -> pd.DataFrame:\n",
    "\t'''\n",
    "\t\tResamples the DataFrame to the specified frequency.\n",
    "\t'''\n",
    "\tproduct_df = product_df.resample(freq).mean()\n",
    "\t# if W, change to first day of week\n",
    "\tif freq == \"W\":\n",
    "\t\tproduct_df.index = product_df.index.to_period(\"W\").to_timestamp()\n",
    "\t# if M, change to first day of month\n",
    "\tif freq == \"M\":\n",
    "\t\tproduct_df.index = product_df.index.to_period(\"M\").to_timestamp()\n",
    "\t# product_df.fillna(method=\"ffill\", inplace=True)\n",
    "\t# product_df.fillna(method=\"bfill\", inplace=True)\n",
    "\treturn product_df\n",
    "\n",
    "product_df_W = resample(product_df, \"W\")\n",
    "product_df_M = resample(product_df, \"M\")\n",
    "product_df_W.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot NEW\n",
    "fig = go.Figure()\n",
    "column_name = \"NEW\" #\"COUNT_NEW\"\n",
    "translate_y = 30\n",
    "# cut all before 2017-01-01 and after 2024-01-01\n",
    "product_df = product_df[\"2017-01-01\":\"2023-12-01\"]\n",
    "# get percentage of non-null values in the column new for each resampled dataframe\n",
    "percent_non_null_original = get_percent_non_null(product_df, column_name)\n",
    "percent_non_null_W = get_percent_non_null(product_df_W, column_name)\n",
    "percent_non_null_M = get_percent_non_null(product_df_M, column_name)\n",
    "title_bottom = f\"Non-missing values: {percent_non_null_original:.2f}% (original), {percent_non_null_W:.2f}% (resampled to W), {percent_non_null_M:.2f}% (resampled to M)\"\n",
    "fig.add_trace(go.Scatter(x=product_df.index, y=product_df[column_name], mode=\"lines+markers\", name=f\"{column_name}\"))\n",
    "fig.add_trace(go.Scatter(x=product_df_W.index, y=product_df_W[column_name] + translate_y, mode=\"lines+markers\", name=f\"{column_name} (W)\"))\n",
    "fig.add_trace(go.Scatter(x=product_df_M.index, y=product_df_M[column_name] + (translate_y * 2), mode=\"lines+markers\", name=f\"{column_name} (M)\"))\n",
    "fig.update_layout(title=f\"Product history of {sample_product['title']} ({sample_product['asin']})<br>{title_bottom}\", xaxis_title=\"Time\", yaxis_title=\"Value\")\n",
    "fig.update_xaxes(range=[\"2017-01-01\", \"2023-12-01\"])\n",
    "fig.update_layout(width=1200, height=500)\n",
    "fig.update_layout(margin=dict(l=0, r=0, t=60, b=0))\n",
    "path_output = os.path.join(path_visualizations_root, f\"sample-product-history-resampling-{sample_product['asin']}.png\")\n",
    "fig.write_image(path_output, scale=2)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_product_info_to_df(product_df: pd.DataFrame, product: dict) -> pd.DataFrame:\n",
    "\t'''\n",
    "\t\tAdds product information to the DataFrame.\n",
    "\t'''\n",
    "\tif \"asin\" in product_df.columns: # assume already added\n",
    "\t\treturn product_df\n",
    "\t# add to front of DataFrame\n",
    "\tproduct_df.insert(0, \"asin\", product[\"asin\"])\n",
    "\tproduct_df.insert(1, \"category\", product[\"category\"])\n",
    "\t# product_df[\"asin\"] = product[\"asin\"]\n",
    "\t# product_df[\"category\"] = product[\"category\"]\n",
    "\treturn product_df\n",
    "\n",
    "product_df = add_product_info_to_df(product_df, category_products[sample_product[\"asin\"]])\n",
    "product_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all products into one DataFrame keeping all rows\n",
    "\n",
    "def merge_category_dfs(product_dfs: list[pd.DataFrame]) -> pd.DataFrame:\n",
    "\t'''\n",
    "\t\tMerges all product DataFrames into one DataFrame.\n",
    "\t'''\n",
    "\t# change index for all dfs to a column named \"time\" as the first column and reset index\n",
    "\tfor product_df in product_dfs:\n",
    "\t\tproduct_df.insert(0, \"time\", product_df.index)\n",
    "\t\tproduct_df.reset_index(drop=True, inplace=True)\n",
    "\t# merge all product dfs into one\n",
    "\tmerged_df = pd.concat(product_dfs, axis=0)\n",
    "\t# reset index\n",
    "\tmerged_df.reset_index(drop=True, inplace=True)\n",
    "\treturn merged_df\n",
    "\n",
    "def get_category_product_dfs(category_name: str, index_categories: dict, category_products: dict) -> list[pd.DataFrame]:\n",
    "\t'''\n",
    "\t\tLoads all products in a category and returns them as DataFrames.\n",
    "\t'''\n",
    "\tproducts = load_products(category_name, index_categories)\n",
    "\tproduct_dfs = []\n",
    "\tfails = []\n",
    "\tfor i, product in enumerate(products.values()):\n",
    "\t\tprint(f\"Processing product {i+1}/{len(products)} ({product['asin']})      \", end=\"\\r\")\n",
    "\t\tproduct_df = csv_to_df(product[\"csv\"])\n",
    "\t\tproduct_df = add_product_info_to_df(product_df, category_products[product[\"asin\"]])\n",
    "\t\tif len(product_df) == 0: # some products have no valid data\n",
    "\t\t\tfails.append(product[\"asin\"])\n",
    "\t\telse:\n",
    "\t\t\tproduct_dfs.append(product_df)\n",
    "\tprint(\"\")\n",
    "\tprint(f\"Successfully loaded {len(product_dfs)}/{len(products)} product dfs\")\n",
    "\treturn product_dfs\n",
    "\n",
    "def get_category_product_dfs_merged(category_name: str, index_categories: dict, category_products: dict) -> pd.DataFrame:\n",
    "\t'''\n",
    "\t\tLoads all products in a category and returns them as a merged DataFrame.\n",
    "\t'''\n",
    "\tprint(f\"Loading product dfs for category '{category_name}'\")\n",
    "\tproduct_dfs = get_category_product_dfs(category_name, index_categories, category_products)\n",
    "\tprint(\"\")\n",
    "\tprint(f\"Merging {len(product_dfs)} product dfs\")\n",
    "\tmerged_df = merge_category_dfs(product_dfs)\n",
    "\tprint(\"\")\n",
    "\tprint(f\"Done! Merged {len(merged_df)} rows\")\n",
    "\treturn merged_df\n",
    "\n",
    "# category = \"PERSONAL_COMPUTER\"\n",
    "category = \"VIDEO_CARD\"\n",
    "merged_df = get_category_product_dfs_merged(category, index_categories, category_products)\n",
    "\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print df info\n",
    "print(merged_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aggregate_timeseries_df(merged_df: pd.DataFrame) -> pd.DataFrame:\n",
    "\t'''\n",
    "\t\tAggregates the merged DataFrame to get a DataFrame mean values for a specific column.\n",
    "\t'''\n",
    "\t# remove asin and category columns\n",
    "\tmerged_df = merged_df.drop(columns=[\"asin\", \"category\"])\n",
    "\t# df_ts = merged_df.groupby(\"time\").mean()\n",
    "\t# NEW, USED, AMAZON, EBAY_NEW_SHIPPING, EBAY_USED_SHIPPING by mean\n",
    "\t# COUNT_NEW, COUNT_USED by sum\n",
    "\tdf_ts = merged_df.groupby(\"time\").agg({\"NEW\": \"mean\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"USED\": \"mean\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"AMAZON\": \"mean\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"EBAY_NEW_SHIPPING\": \"mean\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"EBAY_USED_SHIPPING\": \"mean\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"COUNT_NEW\": \"sum\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"COUNT_USED\": \"sum\"})\n",
    "\n",
    "\t# linear interpolation for NaN values\n",
    "\tdf_ts.interpolate(method=\"time\", inplace=True)\n",
    "\t\n",
    "\t# window = 90\n",
    "\t# # # smooth the data by taking the rolling mean with a window of 20\n",
    "\t# resample_size = 1 # 7 # 1\n",
    "\t# # df_ts = df_ts.rolling(window=int((30 * 3) / resample_size)).mean()\n",
    "\t# # df_ts = df_ts.resample(\"Q\").mean()\n",
    "\t# # df_ts = df_ts.resample(\"W\").mean()\n",
    "\t# # only resample COUNTS to weekly?\n",
    "\t# df_ts[\"NEW\"] = df_ts[\"NEW\"].rolling(window=int(window / resample_size)).mean()\n",
    "\t# df_ts[\"USED\"] = df_ts[\"USED\"].rolling(window=int(window / resample_size)).mean()\n",
    "\t# df_ts[\"AMAZON\"] = df_ts[\"AMAZON\"].rolling(window=int(window / resample_size)).mean()\n",
    "\t# df_ts[\"EBAY_NEW_SHIPPING\"] = df_ts[\"EBAY_NEW_SHIPPING\"].rolling(window=int(window / resample_size)).mean()\n",
    "\t# df_ts[\"EBAY_USED_SHIPPING\"] = df_ts[\"EBAY_USED_SHIPPING\"].rolling(window=int(window / resample_size)).mean()\n",
    "\t# df_ts[\"COUNT_NEW\"] = df_ts[\"COUNT_NEW\"].rolling(window=int(window / resample_size)).sum()\n",
    "\t# df_ts[\"COUNT_USED\"] = df_ts[\"COUNT_USED\"].rolling(window=int(window / resample_size)).sum()\n",
    "\n",
    "\t# use np.convolve to get the rolling average for all columns\n",
    "\twindow = 30 * 3\n",
    "\tavg = np.ones(window) / window\n",
    "\tvalues_avg = np.convolve(df_ts[\"NEW\"].values, avg, mode=\"same\")\n",
    "\tdf_ts[\"NEW\"] = values_avg\n",
    "\tvalues_avg = np.convolve(df_ts[\"USED\"].values, avg, mode=\"same\")\n",
    "\tdf_ts[\"USED\"] = values_avg\n",
    "\tvalues_avg = np.convolve(df_ts[\"AMAZON\"].values, avg, mode=\"same\")\n",
    "\tdf_ts[\"AMAZON\"] = values_avg\n",
    "\tvalues_avg = np.convolve(df_ts[\"EBAY_NEW_SHIPPING\"].values, avg, mode=\"same\")\n",
    "\tdf_ts[\"EBAY_NEW_SHIPPING\"] = values_avg\n",
    "\tvalues_avg = np.convolve(df_ts[\"EBAY_USED_SHIPPING\"].values, avg, mode=\"same\")\n",
    "\tdf_ts[\"EBAY_USED_SHIPPING\"] = values_avg\n",
    "\t# values_avg = np.convolve(df_ts[\"COUNT_NEW\"].values, avg, mode=\"same\")\n",
    "\t# df_ts[\"COUNT_NEW\"] = values_avg\n",
    "\t# values_avg = np.convolve(df_ts[\"COUNT_USED\"].values, avg, mode=\"same\")\n",
    "\t# df_ts[\"COUNT_USED\"] = values_avg\n",
    "\n",
    "\t# Smooth again if not using rolling\n",
    "\twindow = 30\n",
    "\tavg = np.ones(window) / window\n",
    "\tvalues_avg = np.convolve(df_ts[\"NEW\"].values, avg, mode=\"same\")\n",
    "\tdf_ts[\"NEW\"] = values_avg\n",
    "\tvalues_avg = np.convolve(df_ts[\"USED\"].values, avg, mode=\"same\")\n",
    "\tdf_ts[\"USED\"] = values_avg\n",
    "\tvalues_avg = np.convolve(df_ts[\"AMAZON\"].values, avg, mode=\"same\")\n",
    "\tdf_ts[\"AMAZON\"] = values_avg\n",
    "\tvalues_avg = np.convolve(df_ts[\"EBAY_NEW_SHIPPING\"].values, avg, mode=\"same\")\n",
    "\tdf_ts[\"EBAY_NEW_SHIPPING\"] = values_avg\n",
    "\tvalues_avg = np.convolve(df_ts[\"EBAY_USED_SHIPPING\"].values, avg, mode=\"same\")\n",
    "\tdf_ts[\"EBAY_USED_SHIPPING\"] = values_avg\n",
    "\tvalues_avg = np.convolve(df_ts[\"COUNT_NEW\"].values, avg, mode=\"same\")\n",
    "\tdf_ts[\"COUNT_NEW\"] = values_avg\n",
    "\tvalues_avg = np.convolve(df_ts[\"COUNT_USED\"].values, avg, mode=\"same\")\n",
    "\tdf_ts[\"COUNT_USED\"] = values_avg\n",
    "\t\t\n",
    "\n",
    "\t# resample COUNT_NEW and COUNT_USED to weekly frequency with sum\n",
    "\t# df_ts[\"COUNT_NEW\"] = df_ts[\"COUNT_NEW\"].resample(\"W\").sum()\n",
    "\t# df_ts[\"COUNT_USED\"] = df_ts[\"COUNT_USED\"].resample(\"W\").sum()\n",
    "\n",
    "\t# remove all rows before 2019-01-01\n",
    "\t# df_ts = df_ts[df_ts.index >= \"2019-01-01\"]\n",
    "\tdf_ts = df_ts[df_ts.index >= \"2017-01-01\"]\n",
    "\t# remove all after 2024-01-01\n",
    "\t# df_ts = df_ts[df_ts.index <= \"2024-01-01\"]\n",
    "\t# df_ts = df_ts[df_ts.index <= \"2023-11-18\"]\n",
    "\tdf_ts = df_ts[df_ts.index <= \"2023-10-01\"]\n",
    "\treturn df_ts\n",
    "\n",
    "def sample_products(merged_df: pd.DataFrame, sample_size: int) -> pd.DataFrame:\n",
    "\t'''\n",
    "\t\tSamples a number of products from the merged DataFrame.\n",
    "\t'''\n",
    "\t# sample products\n",
    "\tsample_asins = merged_df[\"asin\"].sample(sample_size)\n",
    "\t# filter merged_df\n",
    "\tsample_df = merged_df[merged_df[\"asin\"].isin(sample_asins)]\n",
    "\treturn sample_df\n",
    "\n",
    "def sample_products_with_most_non_nan_values(merged_df: pd.DataFrame, columns: list[str], sample_size: int) -> pd.DataFrame:\n",
    "\t'''\n",
    "\t\tSamples a number of products from the merged DataFrame based on the number of non-NaN values in a list of columns.\n",
    "\t'''\n",
    "\t# get unique products\n",
    "\tunique_asins = merged_df[\"asin\"].unique()\n",
    "\tnon_nan_counts = {asin: 0 for asin in unique_asins}\n",
    "\t# get number of non-NaN values for each product for each column\n",
    "\t# for i, asin in enumerate(unique_asins):\n",
    "\t# \tprint(f\"{i+1}/{len(unique_asins)} ({asin})      \", end=\"\\r\")\n",
    "\t# \t# for column in columns:\n",
    "\t# \t# \tnon_nan_counts[asin] += merged_df[merged_df[\"asin\"] == asin][column].count()\n",
    "\t# \t# do this in a single line\n",
    "\t# \tnon_nan_counts[asin] = merged_df[merged_df[\"asin\"] == asin][columns].count().sum()\n",
    "\t\n",
    "\t# quicker\n",
    "\t# for column in columns:\n",
    "\t# \tnon_nan_counts = merged_df[merged_df[column].notna()][\"asin\"].value_counts().to_dict()\n",
    "\t\n",
    "\t# even quicker\n",
    "\tnon_nan_counts = merged_df[merged_df[columns].notna().all(axis=1)][\"asin\"].value_counts().to_dict()\n",
    "\n",
    "\t# sort products by non-NaN values\n",
    "\tsorted_asins = sorted(non_nan_counts, key=non_nan_counts.get, reverse=True)\n",
    "\t# sample the top products\n",
    "\tsample_asins = sorted_asins[:sample_size]\n",
    "\t# sample products\n",
    "\tsample_df = merged_df[merged_df[\"asin\"].isin(sample_asins)]\n",
    "\treturn sample_df\n",
    "\ta = 0\n",
    "\n",
    "# df_ts = get_aggregate_timeseries_df(merged_df)\n",
    "total_products = len(merged_df[\"asin\"].unique())\n",
    "# sample_size = 500\n",
    "# sample_size = total_products\n",
    "sample_size = 100000 # 100\n",
    "# sampled_df = sample_products(merged_df, sample_size)\n",
    "sampled_df = sample_products_with_most_non_nan_values(merged_df, [\"NEW\", \"USED\", \"AMAZON\"], sample_size)\n",
    "\n",
    "df_ts = get_aggregate_timeseries_df(sampled_df)\n",
    "\n",
    "df_ts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fig(df: pd.DataFrame, title: str) -> go.Figure:\n",
    "\t'''\n",
    "\t\tReturns a plotly figure for a DataFrame.\n",
    "\t'''\n",
    "\t# first row are prices / indices, second row are counts\n",
    "\tfig = make_subplots(rows=2, cols=1, row_heights=[0.8, 0.2], shared_xaxes=True, vertical_spacing=0.02)\n",
    "\tcolors = px.colors.qualitative.Plotly\n",
    "\tfirst_row_columns = [\"NEW\", \"USED\", \"AMAZON\", \"EBAY_NEW_SHIPPING\", \"EBAY_USED_SHIPPING\"]\n",
    "\tsecond_row_columns = [\"COUNT_NEW\", \"COUNT_USED\"]\n",
    "\t# add traces\n",
    "\tfor i, column in enumerate(first_row_columns):\n",
    "\t\tfig.add_trace(go.Scatter(x=df.index, y=df[column], mode=\"lines\", name=column, line=dict(color=colors[i])), row=1, col=1)\n",
    "\tdf_counts = df[second_row_columns]\n",
    "\t# resample to weekly frequency with sum\n",
    "\t# df_counts = df_counts.resample(\"W\").mean()\n",
    "\tfor i, column in enumerate(second_row_columns):\n",
    "\t\tfig.add_trace(go.Bar(x=df_counts.index, y=df_counts[column], name=column, marker=dict(color=colors[i])), row=2, col=1)\n",
    "\t# lower bar spacing\n",
    "\tfig.update_layout(bargap=0)\n",
    "\t# remove bar borders\n",
    "\tfig.update_traces(marker_line_width=0, row=2, col=1)\n",
    "\t# TODO: convert to histogram\n",
    "\t# bottom = px.histogram(df, x=df.index, y=second_row_columns, histfunc=\"sum\")\n",
    "\t# fig.add_trace(bottom, row=2, col=1)\n",
    "\t# change barmode to stacked\n",
    "\tfig.update_layout(barmode=\"stack\")\n",
    "\tfig.update_xaxes(showgrid=True)\n",
    "\t# add \"COUNT_REVIEWS\" to second row with 6th color\n",
    "\t# fig.add_trace(go.Scatter(x=df.index, y=df[\"COUNT_REVIEWS\"], mode=\"lines\", name=\"COUNT_REVIEWS\", line=dict(color=colors[5])), row=2, col=1)\n",
    "\t# update layout\n",
    "\tfig.update_layout(title=title, xaxis_title=\"Time\", yaxis_title=\"Price\")\n",
    "\tfig.update_xaxes(title_text=\"Time\", row=2, col=1)\n",
    "\tfig.update_yaxes(title_text=\"Price trends\", row=1, col=1)\n",
    "\tfig.update_yaxes(title_text=\"Count listings\", row=2, col=1)\n",
    "\t# make 1440x720\n",
    "\tfig.update_layout(width=1280, height=720)\n",
    "\tfig.update_layout(legend=dict(traceorder=\"normal\"))\n",
    "\t# remove first row x-axis title\n",
    "\tfig.update_xaxes(title_text=\"\", row=1, col=1)\n",
    "\t# add events\n",
    "\tfor event in events:\n",
    "\t\tfig = add_vline_annotation(fig, event, textangle=-20)\n",
    "\t# set margin to {\"r\": 0, \"t\": 60, \"b\": 0, \"l\": 0}\n",
    "\tfig.update_layout(margin=dict(r=0, t=60, b=0, l=0))\n",
    "\treturn fig\n",
    "\n",
    "df_ts_copy = df_ts.copy()\n",
    "# df_ts_copy = df_ts_copy.resample(\"M\").mean()\n",
    "\t\n",
    "\n",
    "# fig = get_fig(df_ts_copy, f\"Aggregate price history of category '{category}' using a sample of {sample_size}/{total_products} products\")\n",
    "title = f\"Aggregate product price history trends for category '{category}' for domain '{domain_name_map[domain]}' ({total_products} products)\"\n",
    "fig = get_fig(df_ts_copy, title)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save figure - only do this if you turn off smoothing in get_aggregate_timeseries_df()\n",
    "# filepath = os.path.join(path_visualizations_root, f\"aggregate-price-history-{category}-domain-{domain}-unpolished.png\")\n",
    "# fig.write_image(filepath, scale=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save figure - only do this if you turn on smoothing in get_aggregate_timeseries_df()\n",
    "# filepath = os.path.join(path_visualizations_root, f\"aggregate-price-history-{category}-domain-{domain}-polished.png\")\n",
    "# fig.write_image(filepath, scale=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example DataFrame\n",
    "data = {\n",
    "    'values': [0.5, 0.8, 1.2, 1.5, 1.7],\n",
    "    'dates': ['2017-01-01', '2019-01-03', '2019-01-04', '2019-01-01', '2019-01-05']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df['dates'] = pd.to_datetime(df['dates'])\n",
    "df.set_index('dates', inplace=True)\n",
    "# reorder index\n",
    "df = df.sort_index()\n",
    "\n",
    "dates_epoch = pd.to_datetime(df.index).astype(int) / 10**9\n",
    "\n",
    "\n",
    "# Fixed date\n",
    "fixed_date = '2019-01-02'\n",
    "fixed_date_epoch = pd.to_datetime(fixed_date).timestamp()\n",
    "\n",
    "# Find closest index to fixed date\n",
    "closest_index = dates_epoch - fixed_date_epoch\n",
    "# convert to numpy array\n",
    "closest_index = closest_index.to_numpy()\n",
    "closest_index = np.abs(closest_index).argmin()\n",
    "\n",
    "# Value at closest index (just the number)\n",
    "date_at_index = df.index[closest_index]\n",
    "\n",
    "# Value at closest index (values column)\n",
    "value = df.values[closest_index][0]\n",
    "\n",
    "# Difference in days\n",
    "difference = (df.index[closest_index] - pd.to_datetime(fixed_date)).days\n",
    "\n",
    "# print(\"Closest index to\", fixed_date, \":\", closest_index)\n",
    "# print(\"Closest value to\", fixed_date, \":\", value)\n",
    "# print(\"Difference in days:\", difference)\n",
    "\n",
    "print(f\"Closest index to '{fixed_date}': '{closest_index}'\")\n",
    "print(f\"Date at index to '{fixed_date}': '{date_at_index}'\")\n",
    "print(f\"Difference in days: '{difference}'\")\n",
    "print(f\"Value at index to '{fixed_date}': '{value}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
