{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text mining articles - exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import nltk\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from plotly import graph_objects as go\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_index = \"data/scraped/cnet/index_articles.json\"\n",
    "path_text_root = \"data/scraped/cnet/articles/parsed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load index\n",
    "index = json.load(open(path_index, \"r\"))\n",
    "articles = index[\"articles\"] # key: article id, value: article metadata\n",
    "\n",
    "# Print the number of articles\n",
    "print(f\"Number of articles in the index: {len(articles)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print an example article object\n",
    "print(\"Example article object:\")\n",
    "sample_id = list(articles.keys())[0]\n",
    "print(json.dumps(articles[sample_id], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of article files\n",
    "article_filenames = os.listdir(path_text_root)\n",
    "# Convert to dictionary\n",
    "article_filenames = {f: True for f in article_filenames}\n",
    "print(f\"Number of article files: {len(article_filenames)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use ntlk to get total word count (non-processed word tokens) of all articles\n",
    "total_character_count = 0\n",
    "total_word_count = 0\n",
    "\n",
    "for i, filename in enumerate(article_filenames):\n",
    "\tprint(f\"{i+1}/{len(article_filenames)}       \", end=\"\\r\")\n",
    "\tpath = os.path.join(path_text_root, filename)\n",
    "\twith open(path, \"r\") as file:\n",
    "\t\ttext = file.read()\n",
    "\t\twords = nltk.word_tokenize(text)\n",
    "\t\ttotal_word_count += len(words)\n",
    "\t\ttotal_character_count += len(text)\n",
    "\n",
    "print(f\"Total word count: {total_word_count}\")\n",
    "print(f\"Total character count: {total_character_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When developing it's useful to work with a small subset of articles\n",
    "# as processing all 70k of them takes a while and consumes more than 11 GB of RAM\n",
    "\n",
    "# Suggested VM RAM: 20 GB without sampling articles\n",
    "#                   14 GB is consumed by this process alone\n",
    "\n",
    "# Uncomment the following to sample:\n",
    "# n = 10000\n",
    "# article_filenames = random.sample(list(article_filenames.keys()), n)\n",
    "# print(f\"Sampling {n} articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_article_ids = {}\n",
    "# Remove articles that are not in article_files\n",
    "for article_id in list(articles.keys()):\n",
    "\tif f\"{article_id}.txt\" not in article_filenames:\n",
    "\t\tremoved_article_ids[article_id] = articles[article_id]\n",
    "\t\tdel articles[article_id]\n",
    "\n",
    "print(f\"Removed {len(removed_article_ids)} articles that are not in article_files\")\n",
    "print(f\"Remaining number of articles: {len(articles)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(filepath: str) -> str:\n",
    "\twith open(filepath, \"r\") as f:\n",
    "\t\treturn f.read()\n",
    "\n",
    "sample_text = load_file(os.path.join(path_text_root, f\"{sample_id}.txt\"))\n",
    "print(f\"Example article text (first 100 characters):\\n{sample_text[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "sample_tokens = nltk.word_tokenize(sample_text)\n",
    "\n",
    "# Print sample\n",
    "n = 10\n",
    "print(f\"Example article tokens (first {n} tokens out of {len(sample_tokens)}):\")\n",
    "for i, token in enumerate(sample_tokens[:n]):\n",
    "\tprint(f\"{i+1}. '{token}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get stopwords\n",
    "sample_stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Print sample stopwords\n",
    "n = 10\n",
    "print(f\"Example stopwords (first {n} stopwords out of {len(sample_stopwords)}):\")\n",
    "for i, token in enumerate(sample_stopwords[:n]):\n",
    "\tprint(f\"{i+1}. '{token}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "sample_stopwords_set = set(sample_stopwords)\n",
    "sample_tokens_no_stopwords = [token for token in sample_tokens if token not in sample_stopwords_set]\n",
    "\n",
    "# Print sample tokens without stopwords\n",
    "n = 10\n",
    "print(f\"Example tokens without stopwords (first {n} tokens out of {len(sample_tokens_no_stopwords)}):\")\n",
    "for i, token in enumerate(sample_tokens_no_stopwords[:n]):\n",
    "\tprint(f\"{i+1}. '{token}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stemming\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "sample_tokens_stemmed = [stemmer.stem(token.lower()) for token in sample_tokens_no_stopwords]\n",
    "\n",
    "# Print sample tokens without stemming\n",
    "n = 10\n",
    "print(f\"Example tokens without stemming (first {n} tokens out of {len(sample_tokens_stemmed)}):\")\n",
    "for i, token in enumerate(sample_tokens_stemmed[:n]):\n",
    "\tprint(f\"{i+1}. '{token}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation with nltk\n",
    "sample_tokens_no_punctuation = [token for token in sample_tokens_stemmed if token.isalnum()]\n",
    "\n",
    "# Print sample tokens without punctuation\n",
    "n = 10\n",
    "print(f\"Example tokens without punctuation (first {n} tokens out of {len(sample_tokens_no_punctuation)}):\")\n",
    "for i, token in enumerate(sample_tokens_no_punctuation[:n]):\n",
    "\tprint(f\"{i+1}. '{token}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tokens_processed = sample_tokens_no_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of unique tokens\n",
    "sample_tokens_unique = set(sample_tokens_processed)\n",
    "print(f\"Number of tokens: {len(sample_tokens_processed)}\")\n",
    "print(f\"Number of unique tokens: {len(sample_tokens_unique)}\")\n",
    "# Get token frequency\n",
    "sample_token_freq = nltk.FreqDist(sample_tokens_processed)\n",
    "print(f\"Most common tokens: {sample_token_freq.most_common(10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_tokens(text: str) -> list:\n",
    "\t# Tokenize\n",
    "\ttokens = nltk.word_tokenize(text)\n",
    "\t# Remove stopwords\n",
    "\tstopwords_set = set(nltk.corpus.stopwords.words('english'))\n",
    "\ttokens = [token for token in tokens if token not in stopwords_set]\n",
    "\t# Stem\n",
    "\tstemmer = nltk.stem.PorterStemmer()\n",
    "\ttokens = [stemmer.stem(token.lower()) for token in tokens]\n",
    "\t# Remove punctuation\n",
    "\ttokens = [token for token in tokens if token.isalnum()]\n",
    "\treturn tokens\n",
    "\n",
    "sample_tokens_processed = get_clean_tokens(sample_text)\n",
    "\n",
    "# Get number of unique tokens\n",
    "sample_tokens_unique = set(sample_tokens_processed)\n",
    "print(f\"Number of tokens: {len(sample_tokens_processed)}\")\n",
    "print(f\"Number of unique tokens: {len(sample_tokens_unique)}\")\n",
    "# Get token frequency\n",
    "sample_token_freq = nltk.FreqDist(sample_tokens_processed)\n",
    "print(f\"Most common tokens: {sample_token_freq.most_common(10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all tokens for all articles\n",
    "articles_tokens = {} # key: article id, value: list of tokens\n",
    "time_start = time.time()\n",
    "\n",
    "for i, article_id in enumerate(articles):\n",
    "\t# print(f\"{i+1}/{len(articles)}: {article_id}          \", end=\"\\r\")\n",
    "\ttime_elapsed = time.time() - time_start\n",
    "\ttime_per_article = time_elapsed / (i+1)\n",
    "\ttime_remaining = time_per_article * (len(articles) - i)\n",
    "\tprint(f\"{i+1}/{len(articles)}: {article_id}   ; Time remaining: {time_remaining:.2f} seconds          \", end=\"\\r\")\n",
    "\tarticle_text = load_file(os.path.join(path_text_root, f\"{article_id}.txt\"))\n",
    "\tarticles_tokens[article_id] = get_clean_tokens(article_text)\n",
    "print(\"\")\n",
    "time_elapsed = time.time() - time_start\n",
    "print(f\"Elapsed time: {time_elapsed:.2f} seconds\")\n",
    "\n",
    "# Print the total number of tokens\n",
    "total_tokens = 0\n",
    "for article_id in articles_tokens:\n",
    "\ttotal_tokens += len(articles_tokens[article_id])\n",
    "print(f\"Total number of tokens: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of tokens: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Iterate over all tokens and count the number of articles that contain each token\n",
    "# token_article_sets = {} # key: token, value: set of article ids\n",
    "\n",
    "# for i, article_id in enumerate(articles_tokens):\n",
    "# \tprint(f\"{i+1}/{len(articles_tokens)}: {article_id}          \", end=\"\\r\")\n",
    "# \tfor token in articles_tokens[article_id]:\n",
    "# \t\tif token not in token_article_sets:\n",
    "# \t\t\ttoken_article_sets[token] = set()\n",
    "# \t\ttoken_article_sets[token].add(article_id)\n",
    "# print(\"\")\n",
    "\n",
    "# # Get the number of articles that contain each token\n",
    "# token_article_counts = {} # key: token, value: number of articles that contain the token\n",
    "# for token in token_article_sets:\n",
    "# \ttoken_article_counts[token] = len(token_article_sets[token])\n",
    "\n",
    "# # Get a sorted list of tokens by their number of articles\n",
    "# sorted_tokens = sorted(token_article_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# # Print the most common tokens\n",
    "# n = 10\n",
    "# print(f\"Most common tokens (first {n} tokens out of {len(sorted_tokens)}):\")\n",
    "# for i, (token, count) in enumerate(sorted_tokens[:n]):\n",
    "# \tprint(f\"{i+1}. '{token}': {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_word(word: str) -> str:\n",
    "\t# Stem\n",
    "\tstemmer = nltk.stem.PorterStemmer()\n",
    "\tword = stemmer.stem(word.lower())\n",
    "\t# Remove punctuation\n",
    "\tif not word.isalnum():\n",
    "\t\traise Exception(f\"Word '{word}' is not alphanumeric\")\n",
    "\treturn word\n",
    "\n",
    "sample_processed_word = process_word(\"shorage\")\n",
    "print(f\"Example processed word: '{sample_processed_word}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: process n-grams (bigrams, trigrams, etc.) - to make \"chip shortage\" queryable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get article ids that contain a specific word\n",
    "\n",
    "# def get_article_ids_containing_word(word: str) -> list:\n",
    "# \tword = process_word(word)\n",
    "# \tif word not in token_article_sets:\n",
    "# \t\treturn []\n",
    "# \treturn list(token_article_sets[word])\n",
    "\n",
    "# sample_word = \"semiconductor\" # \"chip\" # \"shortage\"\n",
    "# sample_article_ids = get_article_ids_containing_word(sample_word)\n",
    "# print(f\"Word '{sample_word}' appears in {len(sample_article_ids)} articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # From the article ids, get the article objects and use \"dateCreated\" to count how many articles which contain the word were published each day\n",
    "# # \"dateCreated\" is formatted as \"2020-01-04 12:00:03\"\n",
    "\n",
    "# def get_article_counts_by_date(article_ids: list) -> dict:\n",
    "# \tarticle_counts_by_date = {} # key: date, value: number of articles published on that date\n",
    "# \tfor article_id in article_ids:\n",
    "# \t\tdate = articles[article_id][\"dateCreated\"].split(\" \")[0]\n",
    "# \t\tif date not in article_counts_by_date:\n",
    "# \t\t\tarticle_counts_by_date[date] = 0\n",
    "# \t\tarticle_counts_by_date[date] += 1\n",
    "# \treturn article_counts_by_date\n",
    "\n",
    "# sample_article_counts_by_date = get_article_counts_by_date(sample_article_ids)\n",
    "\n",
    "# # Get a sorted list of dates (oldest to newest)\n",
    "# sorted_dates = sorted(sample_article_counts_by_date.items(), key=lambda x: x[0])\n",
    "\n",
    "# # Print the number of articles containing the word for each date\n",
    "# n = 10\n",
    "# print(f\"Number of articles containing '{sample_word}' (first {n} dates out of {len(sorted_dates)}):\")\n",
    "# for i, (date, count) in enumerate(sorted_dates[:n]):\n",
    "# \tprint(f\"{i+1}. '{date}': {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert the sorted list of article counts to a dataframe\n",
    "\n",
    "# df = pd.DataFrame(sorted_dates, columns=[\"date\", \"count\"])\n",
    "# df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "# df = df.set_index(\"date\")\n",
    "# df = df.resample(\"D\").sum() # resample to daily frequency\n",
    "# df = df.sort_index()\n",
    "\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the number of articles containing the word for each date as a line chart\n",
    "\n",
    "# # Get SMA (simple moving average) of the number of articles\n",
    "# window_size = 20\n",
    "# df[\"sma\"] = df[\"count\"].rolling(window_size).mean()\n",
    "# df[\"ema\"] = df[\"count\"].ewm(span=window_size).mean()\n",
    "\n",
    "# fig = go.Figure()\n",
    "\n",
    "# fig.add_trace(go.Scatter(\n",
    "# \tx=df.index,\n",
    "# \ty=df[\"count\"],\n",
    "# \tname=\"Count\",\n",
    "# ))\n",
    "\n",
    "# # fig.add_trace(go.Scatter(\n",
    "# # \tx=df.index,\n",
    "# # \ty=df[\"sma\"],\n",
    "# # \tname=\"SMA\",\n",
    "# # ))\n",
    "\n",
    "# fig.add_trace(go.Scatter(\n",
    "# \tx=df.index,\n",
    "# \ty=df[\"ema\"],\n",
    "# \tname=\"EMA\",\n",
    "# ))\n",
    "\n",
    "# fig.update_layout(\n",
    "# \ttitle=f\"Number of articles containing '{sample_word}' per day\",\n",
    "# \txaxis_title=\"Date\",\n",
    "# \tyaxis_title=\"Number of articles\",\n",
    "# )\n",
    "\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get n-grams from articles_tokens\n",
    "ngrams = {}\n",
    "\n",
    "def get_ngrams(tokens: list, n: int) -> tuple:\n",
    "\treturn tuple(nltk.ngrams(tokens, n))\n",
    "\n",
    "# unigrams\n",
    "ngrams[1] = {}\n",
    "print(f\"Getting unigrams...\")\n",
    "for i, article_id in enumerate(articles_tokens):\n",
    "\tprint(f\"{i+1}/{len(articles_tokens)}: {article_id}          \", end=\"\\r\")\n",
    "\tngrams[1][article_id] = get_ngrams(articles_tokens[article_id], 1)\n",
    "print(\"\")\n",
    "\n",
    "# bigrams\n",
    "ngrams[2] = {}\n",
    "print(f\"Getting bigrams...\")\n",
    "for i, article_id in enumerate(articles_tokens):\n",
    "\tprint(f\"{i+1}/{len(articles_tokens)}: {article_id}          \", end=\"\\r\")\n",
    "\tngrams[2][article_id] = get_ngrams(articles_tokens[article_id], 2)\n",
    "print(\"\")\n",
    "\n",
    "# trigrams\n",
    "ngrams[3] = {}\n",
    "print(f\"Getting trigrams...\")\n",
    "for i, article_id in enumerate(articles_tokens):\n",
    "\tprint(f\"{i+1}/{len(articles_tokens)}: {article_id}          \", end=\"\\r\")\n",
    "\tngrams[3][article_id] = get_ngrams(articles_tokens[article_id], 3)\n",
    "print(\"\")\n",
    "\n",
    "print(f\"Number of unigrams: {len(ngrams[1])}\")\n",
    "print(f\"Number of bigrams: {len(ngrams[2])}\")\n",
    "print(f\"Number of trigrams: {len(ngrams[3])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(ngram: tuple) -> tuple:\n",
    "\treturn tuple([process_word(word) for word in ngram])\n",
    "\n",
    "sample_ngram = \"chip shortage\"\n",
    "sample_ngram = tuple(sample_ngram.split(\" \"))\n",
    "sample_processed_ngram = process_query(sample_ngram)\n",
    "print(f\"'{sample_ngram}' -> '{sample_processed_ngram}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of ngrams to article ids\n",
    "\n",
    "ngrams_search = {}\n",
    "\n",
    "for n in ngrams:\n",
    "\tprint(f\"Processing {n}-grams...\")\n",
    "\tngrams_search[n] = {}\n",
    "\tfor i, article_id in enumerate(ngrams[n]):\n",
    "\t\tprint(f\"{i+1}/{len(ngrams[n])}: {article_id}          \", end=\"\\r\")\n",
    "\t\tfor ngram in ngrams[n][article_id]:\n",
    "\t\t\tngram_joint = \" \".join(ngram)\n",
    "\t\t\tif ngram_joint not in ngrams_search[n]:\n",
    "\t\t\t\tngrams_search[n][ngram_joint] = set()\n",
    "\t\t\tngrams_search[n][ngram_joint].add(article_id)\n",
    "\tprint(\"\")\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Clear up some memory - TODO - fix this\n",
    "# del ngrams\n",
    "# ngrams = ngrams_search\n",
    "# del ngrams_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_ids_containing_search_term(search_term: str) -> list:\n",
    "\t# Convert to tuple\n",
    "\tsearch_term_tuple = tuple(search_term.split(\" \"))\n",
    "\tn = len(search_term_tuple)\n",
    "\t# Process\n",
    "\tsearch_term_ngrams = \" \".join(process_query(search_term_tuple))\n",
    "\tif n not in ngrams_search:\n",
    "\t\treturn []\n",
    "\tif search_term_ngrams not in ngrams_search[n]:\n",
    "\t\treturn []\n",
    "\treturn list(ngrams_search[n][search_term_ngrams])\n",
    "\n",
    "sample_search_term = \"chip shortage\"\n",
    "result_article_ids = get_article_ids_containing_search_term(sample_search_term)\n",
    "print(f\"Search term '{sample_search_term}' appears in {len(result_article_ids)} articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_article_ids = set()\n",
    "\n",
    "search_terms = []\n",
    "\n",
    "# search_terms += [\n",
    "# \t# \"chip\",\n",
    "# \t\"chip shortage\",\n",
    "# \t\"semiconductor shortage\",\n",
    "# \t\"chip supply shortage\",\n",
    "# ]\n",
    "\n",
    "search_terms += [\n",
    "\t\"covid19\",\n",
    "\t\"covid\",\n",
    "\t\"coronavirus\",\n",
    "\t\"pandemic\",\n",
    "\t\"lockdown\",\n",
    "\t# \"quarantine\",\n",
    "\t\"social distancing\",\n",
    "\t\"wfh\",\n",
    "\t\"work from home\",\n",
    "]\n",
    "\n",
    "# search_terms += [\n",
    "# \t\"supply chain\",\n",
    "# \t\"supply chain disruption\",\n",
    "# \t\"supply chain shortages\",\n",
    "# ]\n",
    "\n",
    "# search_terms += [\n",
    "# \t\"suez canal\",\n",
    "# \t\"container ship\",\n",
    "# \t\"ever given\",\n",
    "# \t\"evergreen marine\",\n",
    "# ]\n",
    "\n",
    "# search_terms += [\n",
    "# \t\"low supply\",\n",
    "# \t\"high demand\",\n",
    "# \t\"high cpu prices\",\n",
    "# \t\"high gpu prices\",\n",
    "# \t\"high ssd prices\",\n",
    "# \t\"high ram prices\",\n",
    "# \t\"high memory prices\",\n",
    "# \t\"high storage prices\",\n",
    "# \t\"high component prices\",\n",
    "# \t\"high electronics prices\",\n",
    "# \t\"high computer prices\",\n",
    "# \t\"high laptop prices\",\n",
    "# \t\"high pc prices\",\n",
    "# \t\"high smartphone prices\",\n",
    "# ]\n",
    "\n",
    "# search_terms += [\n",
    "# \t\"global chip shortage\",\n",
    "# \t\"global semiconductor shortage\",\n",
    "# \t\"global chip supply shortage\",\n",
    "# \t\"chip shortage\",\n",
    "# \t\"semiconductor shortage\",\n",
    "# \t\"chip supply shortage\",\n",
    "# \t\"supply chain\",\n",
    "# \t\"supply chain disruption\",\n",
    "# \t\"supply chain shortages\",\n",
    "# \t\"rare earth minerals\",\n",
    "# \t\"rare earth metals\",\n",
    "# \t\"rare gas\",\n",
    "# \t\"extreme weather\",\n",
    "# \t\"trade war\",\n",
    "# ]\n",
    "\n",
    "# search_terms = [\n",
    "# \t\"gdpr\",\n",
    "# \t\"european commission\",\n",
    "# ]\n",
    "\n",
    "article_ids_by_search_term = {}\n",
    "for search_term in search_terms:\n",
    "\tarticle_ids = get_article_ids_containing_search_term(search_term)\n",
    "\tarticle_ids_by_search_term[search_term] = article_ids\n",
    "\tprint(f\"Search term '{search_term}' appears in {len(article_ids)} articles\")\n",
    "\tjoint_article_ids = joint_article_ids.union(set(article_ids))\n",
    "print(\"\")\n",
    "\n",
    "print(f\"Number of articles containing any of the search terms: {len(joint_article_ids)} ({len(joint_article_ids) / len(articles) * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_of_article_counts_by_date(article_ids: set) -> pd.DataFrame:\n",
    "\t# Get df of article counts by date where the article contains any of the search terms using joint_article_ids set\n",
    "\tdf = pd.DataFrame()\n",
    "\tdf[\"date\"] = [datetime.strptime(articles[article_id][\"dateCreated\"].split(\" \")[0], \"%Y-%m-%d\") for article_id in article_ids]\n",
    "\tdf = df.set_index(\"date\")\n",
    "\t\n",
    "\t# Sort by date\n",
    "\tdf = df.sort_index()\n",
    "\t\n",
    "\t# Get counts\n",
    "\tdf = df.resample(\"D\").size() # resample to daily frequency\n",
    "\n",
    "\t# Add all missing dates from 2019-01-01 to 2023-12-31 if they are not already in the df index and set count to 0\n",
    "\tstart_date = \"2019-01-01\"\n",
    "\tend_date = \"2023-12-31\"\n",
    "\tall_dates = pd.date_range(start=start_date, end=end_date)\n",
    "\tdf = df.reindex(all_dates, fill_value=0)\n",
    "\n",
    "\t# make a new column with count\n",
    "\tdf = df.reset_index()\n",
    "\tdf.columns = [\"date\", \"count\"]\n",
    "\tdf = df.set_index(\"date\")\n",
    "\n",
    "\t# Get SMA\n",
    "\twindow_size = 20\n",
    "\tdf[\"sma\"] = df[\"count\"].rolling(window_size).mean()\n",
    "\n",
    "\t# # Get EMA\n",
    "\tdf[\"ema\"] = df[\"count\"].ewm(span=window_size).mean()\n",
    "\n",
    "\t# Comment these two out to get daily data (in-depth exploration why some days have so many articles)\n",
    "\t# resample to other frequencies\n",
    "\t# df = df.resample(\"M\").sum()\n",
    "\t# remove day component from index\n",
    "\t# df.index = df.index.map(lambda x: x.strftime(\"%Y-%m\"))\n",
    "\n",
    "\treturn df\n",
    "\n",
    "df = get_df_of_article_counts_by_date(joint_article_ids)\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the number of articles containing the word for each date as a line chart\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# fig.add_trace(go.Scatter(\n",
    "# \tx=df.index,\n",
    "# \ty=df[\"count\"],\n",
    "# \tname=\"Count\",\n",
    "# ))\n",
    "\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "\tx=df.index,\n",
    "\ty=df[\"count\"],\n",
    "\tname=\"Count\",\n",
    "))\n",
    "\n",
    "# fig.add_trace(go.Scatter(\n",
    "# \tx=df.index,\n",
    "# \ty=df[\"sma\"],\n",
    "# \tname=\"SMA\",\n",
    "# ))\n",
    "\n",
    "# fig.add_trace(go.Scatter(\n",
    "# \tx=df.index,\n",
    "# \ty=df[\"ema\"],\n",
    "# \tname=\"EMA\",\n",
    "# ))\n",
    "\n",
    "fig.update_layout(\n",
    "\ttitle=f\"Number of articles containing any of the search terms per month\",\n",
    "\txaxis_title=\"Date\",\n",
    "\tyaxis_title=\"Number of articles\",\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO weight by the number of daily articles ?\n",
    "\n",
    "# TODO: plot multiple search terms on the same chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print search terms and their counts\n",
    "print(\"Search terms and their counts:\")\n",
    "for search_term in search_terms:\n",
    "\tprint(f\"'{search_term}': {len(article_ids_by_search_term[search_term])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_search_term_counts_for_date(article_ids_by_search_term: dict, date: str):\n",
    "\tprint(f\"Search term counts for date '{date}':\")\n",
    "\tfor search_term in search_terms:\n",
    "\t\tcount = 0\n",
    "\t\tfor article_id in article_ids_by_search_term[search_term]:\n",
    "\t\t\tif articles[article_id][\"dateCreated\"].split(\" \")[0] == date:\n",
    "\t\t\t\tcount += 1\n",
    "\t\tprint(f\"'{search_term}': {count}\")\n",
    "\n",
    "# Print search term counts for a specific date\n",
    "print_search_term_counts_for_date(article_ids_by_search_term, \"2022-03-04\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_articles_for_search_term_date(search_term: str, date: str):\n",
    "\tarticle_ids = article_ids_by_search_term[search_term]\n",
    "\tarticle_ids = [article_id for article_id in article_ids if articles[article_id][\"dateCreated\"].split(\" \")[0] == date]\n",
    "\tprint(f\"Search term '{search_term}' appears in {len(article_ids)} articles on {date}\")\n",
    "\tfor article_id in article_ids:\n",
    "\t\tprint(f\"- {article_id}: {articles[article_id]['title']}\")\n",
    "\n",
    "print_articles_for_search_term_date(\"coronavirus\", \"2022-03-04\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_id = \"576119ec-c911-41e9-bc91-1b4ea4ffb3a9\"\n",
    "\n",
    "article_text = load_file(os.path.join(path_text_root, f\"{article_id}.txt\"))\n",
    "\n",
    "print(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO filter out paragraphs that start with \"see also\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
