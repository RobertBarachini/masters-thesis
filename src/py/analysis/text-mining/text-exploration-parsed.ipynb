{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text mining articles - exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When developing it's useful to work with a small subset of articles\n",
    "# as processing all 70k of them takes a while and consumes more than 11 GB of RAM\n",
    "\n",
    "# Suggested VM RAM: 20 GB without sampling articles\n",
    "#                   14 GB is consumed by this process alone\n",
    "\n",
    "# Uncomment the following to sample:\n",
    "# n = 10000\n",
    "# article_filenames = random.sample(list(article_filenames.keys()), n)\n",
    "# print(f\"Sampling {n} articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import nltk\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from plotly import graph_objects as go\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_index = \"data/scraped/cnet/index_articles.json\"\n",
    "path_text_root = \"data/scraped/cnet/articles/parsed\"\n",
    "path_parsed_tokens = \"data/scraped/cnet/parsed-articles-index.json\"\n",
    "path_output_root = \"data/analysis/text-mining\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load index\n",
    "index = json.load(open(path_index, \"r\"))\n",
    "articles = index[\"articles\"] # key: article id, value: article metadata\n",
    "\n",
    "# Print the number of articles\n",
    "print(f\"Number of articles in the index: {len(articles)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of article files\n",
    "article_filenames = os.listdir(path_text_root)\n",
    "# Convert to dictionary\n",
    "article_filenames = {f: True for f in article_filenames}\n",
    "print(f\"Number of article files: {len(article_filenames)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_article_ids = {}\n",
    "# Remove articles that are not in article_files\n",
    "for article_id in list(articles.keys()):\n",
    "\tif f\"{article_id}.txt\" not in article_filenames:\n",
    "\t\tremoved_article_ids[article_id] = articles[article_id]\n",
    "\t\tdel articles[article_id]\n",
    "\n",
    "print(f\"Removed {len(removed_article_ids)} articles that are not in article_files\")\n",
    "print(f\"Remaining number of articles: {len(articles)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all tokens for all articles\n",
    "print(f\"Loading tokens from {path_parsed_tokens}\")\n",
    "time_start = time.time()\n",
    "articles_tokens = json.load(open(path_parsed_tokens, \"r\"))\n",
    "time_elapsed = time.time() - time_start\n",
    "print(f\"Elapsed time: {time_elapsed:.2f} seconds\")\n",
    "\n",
    "# Print the total number of tokens\n",
    "total_tokens = 0\n",
    "for article_id in articles_tokens:\n",
    "\ttotal_tokens += len(articles_tokens[article_id])\n",
    "print(f\"Total number of tokens: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_word(word: str) -> str:\n",
    "\t# Stem\n",
    "\tstemmer = nltk.stem.PorterStemmer()\n",
    "\tword = stemmer.stem(word.lower())\n",
    "\t# Remove punctuation\n",
    "\tif not word.isalnum():\n",
    "\t\traise Exception(f\"Word '{word}' is not alphanumeric\")\n",
    "\treturn word\n",
    "\n",
    "sample_processed_word = process_word(\"shortage\")\n",
    "print(f\"Example processed word: '{sample_processed_word}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get n-grams from articles_tokens\n",
    "time_start = time.time()\n",
    "ngrams = {}\n",
    "\n",
    "def get_ngrams(tokens: list, n: int) -> tuple:\n",
    "\treturn tuple(nltk.ngrams(tokens, n))\n",
    "\n",
    "# unigrams\n",
    "ngrams[1] = {}\n",
    "print(f\"Getting unigrams...\")\n",
    "for i, article_id in enumerate(articles_tokens):\n",
    "\tprint(f\"{i+1}/{len(articles_tokens)}: {article_id}          \", end=\"\\r\")\n",
    "\tngrams[1][article_id] = get_ngrams(articles_tokens[article_id], 1)\n",
    "print(\"\")\n",
    "\n",
    "# bigrams\n",
    "ngrams[2] = {}\n",
    "print(f\"Getting bigrams...\")\n",
    "for i, article_id in enumerate(articles_tokens):\n",
    "\tprint(f\"{i+1}/{len(articles_tokens)}: {article_id}          \", end=\"\\r\")\n",
    "\tngrams[2][article_id] = get_ngrams(articles_tokens[article_id], 2)\n",
    "print(\"\")\n",
    "\n",
    "# trigrams\n",
    "ngrams[3] = {}\n",
    "print(f\"Getting trigrams...\")\n",
    "for i, article_id in enumerate(articles_tokens):\n",
    "\tprint(f\"{i+1}/{len(articles_tokens)}: {article_id}          \", end=\"\\r\")\n",
    "\tngrams[3][article_id] = get_ngrams(articles_tokens[article_id], 3)\n",
    "print(\"\")\n",
    "\n",
    "time_elapsed = time.time() - time_start\n",
    "print(f\"Elapsed time: {time_elapsed:.2f} seconds\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count_of_ngrams(ngrams: dict) -> dict:\n",
    "\tcounts = {}\n",
    "\tfor n in ngrams:\n",
    "\t\tcounts[n] = 0\n",
    "\t\tfor article_id in ngrams[n]:\n",
    "\t\t\tcounts[n] += len(ngrams[n][article_id])\n",
    "\treturn counts\n",
    "\n",
    "ngram_counts = get_count_of_ngrams(ngrams)\n",
    "print(f\"Number of unigrams: {ngram_counts[1]}\")\n",
    "print(f\"Number of bigrams: {ngram_counts[2]}\")\n",
    "print(f\"Number of trigrams: {ngram_counts[3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(ngram: tuple) -> tuple:\n",
    "\treturn tuple([process_word(word) for word in ngram])\n",
    "\n",
    "sample_ngram = \"chip shortage\"\n",
    "sample_ngram = tuple(sample_ngram.split(\" \"))\n",
    "sample_processed_ngram = process_query(sample_ngram)\n",
    "print(f\"'{sample_ngram}' -> '{sample_processed_ngram}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of ngrams to article ids\n",
    "\n",
    "ngrams_search = {}\n",
    "time_start = time.time()\n",
    "\n",
    "for n in ngrams:\n",
    "\tprint(f\"Processing {n}-grams...\")\n",
    "\tngrams_search[n] = {}\n",
    "\tfor i, article_id in enumerate(ngrams[n]):\n",
    "\t\tprint(f\"{i+1}/{len(ngrams[n])}: {article_id}          \", end=\"\\r\")\n",
    "\t\tfor ngram in ngrams[n][article_id]:\n",
    "\t\t\tngram_joint = \" \".join(ngram)\n",
    "\t\t\tif ngram_joint not in ngrams_search[n]:\n",
    "\t\t\t\tngrams_search[n][ngram_joint] = set()\n",
    "\t\t\tngrams_search[n][ngram_joint].add(article_id)\n",
    "\tprint(\"\")\n",
    "\n",
    "time_elapsed = time.time() - time_start\n",
    "print(f\"Elapsed time: {time_elapsed:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_ids_containing_search_term(search_term: str) -> list:\n",
    "\t# Convert to tuple\n",
    "\tsearch_term_tuple = tuple(search_term.split(\" \"))\n",
    "\tn = len(search_term_tuple)\n",
    "\t# Process\n",
    "\tsearch_term_ngrams = \" \".join(process_query(search_term_tuple))\n",
    "\tif n not in ngrams_search:\n",
    "\t\treturn []\n",
    "\tif search_term_ngrams not in ngrams_search[n]:\n",
    "\t\treturn []\n",
    "\treturn list(ngrams_search[n][search_term_ngrams])\n",
    "\n",
    "sample_search_term = \"chip shortage\"\n",
    "result_article_ids = get_article_ids_containing_search_term(sample_search_term)\n",
    "print(f\"Search term '{sample_search_term}' appears in {len(result_article_ids)} articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_article_ids = set()\n",
    "\n",
    "search_terms = []\n",
    "\n",
    "# search_terms += [\n",
    "# \t# \"chip\",\n",
    "# \t\"chip shortage\",\n",
    "# \t\"chip crisis\",\n",
    "# \t\"semiconductor shortage\",\n",
    "# \t\"chip supply shortage\",\n",
    "# \t\"microchip shortage\",\n",
    "# \t\"chip scarcity\",\n",
    "# \t\"automotive chip shortage\",\n",
    "# ]\n",
    "\n",
    "search_terms += [\n",
    "\t\"covid19\",\n",
    "\t\"covid\",\n",
    "\t\"coronavirus\",\n",
    "\t\"pandemic\",\n",
    "\t\"lockdown\",\n",
    "\t\"quarantine\",\n",
    "\t\"social distancing\",\n",
    "\t\"wfh\",\n",
    "\t\"work from home\",\n",
    "\t\"sars\"\n",
    "]\n",
    "\n",
    "# search_terms += [\n",
    "# \t\"supply chain\",\n",
    "# \t\"supply chain disruption\",\n",
    "# \t\"supply chain shortages\",\n",
    "# ]\n",
    "\n",
    "# search_terms += [\n",
    "# \t\"suez canal\",\n",
    "# \t\"container ship\",\n",
    "# \t\"ever given\",\n",
    "# \t\"evergreen marine\",\n",
    "# ]\n",
    "\n",
    "# search_terms += [\n",
    "# \t\"low supply\",\n",
    "# \t\"high demand\",\n",
    "# \t\"high cpu prices\",\n",
    "# \t\"high gpu prices\",\n",
    "# \t\"high ssd prices\",\n",
    "# \t\"high ram prices\",\n",
    "# \t\"high memory prices\",\n",
    "# \t\"high storage prices\",\n",
    "# \t\"high component prices\",\n",
    "# \t\"high electronics prices\",\n",
    "# \t\"high computer prices\",\n",
    "# \t\"high laptop prices\",\n",
    "# \t\"high pc prices\",\n",
    "# \t\"high smartphone prices\",\n",
    "# ]\n",
    "\n",
    "# Extended\n",
    "# search_terms += [\n",
    "# \t\"low supply\",\n",
    "# \t\"high demand\",\n",
    "# \t\"high prices\",\n",
    "# \t\"price hike\",\n",
    "# \t\"gpu scalping\",\n",
    "# \t\"scarcity\",\n",
    "# ]\n",
    "# search_terms += [\n",
    "# \t\"global chip shortage\",\n",
    "# \t\"global semiconductor shortage\",\n",
    "# \t\"global chip supply shortage\",\n",
    "# \t\"chip shortage\",\n",
    "# \t\"semiconductor shortage\",\n",
    "# \t\"chip supply shortage\",\n",
    "# \t\"supply chain\",\n",
    "# \t\"supply chain disruption\",\n",
    "# \t\"supply chain shortages\",\n",
    "# \t\"rare earth minerals\",\n",
    "# \t\"rare earth metals\",\n",
    "# \t\"rare gas\",\n",
    "# \t\"extreme weather\",\n",
    "# \t\"trade war\",\n",
    "# \t\"taiwan\",\n",
    "# \t\"tsmc\",\n",
    "# \t\"ultrapure water\",\n",
    "# \t\"neon gas\",\n",
    "# \t\"ukraine war\",\n",
    "# \t\"car prices\",\n",
    "# ]\n",
    "\n",
    "# search_terms += [\n",
    "# \t\"gdpr\",\n",
    "# \t\"european commission\",\n",
    "# ]\n",
    "\n",
    "# search_terms += [\n",
    "# \t\"crypto\",\n",
    "# \t\"crypto price\",\n",
    "# \t\"cryptocurrency\",\n",
    "# \t\"bitcoin\",\n",
    "# \t\"ethereum\",\n",
    "# \t\"blockchain\",\n",
    "# \t\"cryptomining\",\n",
    "# \t\"crypto mining\",\n",
    "# \t\"cryptocurrency mining\",\n",
    "# \t\"scalping\",\n",
    "# \t\"scalper\",\n",
    "# \t\"nft\",\n",
    "# \t\"defi\",\n",
    "# \t\"decentralized finance\",\n",
    "# \t\"stablecoin\",\n",
    "# \t\"crypto wallet\",\n",
    "# \t\"crypto miner\",\n",
    "# ]\n",
    "\n",
    "\n",
    "article_ids_by_search_term = {}\n",
    "for search_term in search_terms:\n",
    "\tarticle_ids = get_article_ids_containing_search_term(search_term)\n",
    "\tarticle_ids_by_search_term[search_term] = article_ids\n",
    "\t# print(f\"Search term '{search_term}' appears in {len(article_ids)} articles\")\n",
    "\tjoint_article_ids = joint_article_ids.union(set(article_ids))\n",
    "# print(\"\")\n",
    "\n",
    "# print(f\"Number of articles containing any of the search terms: {len(joint_article_ids)} ({len(joint_article_ids) / len(articles) * 100:.2f}%)\")\n",
    "\n",
    "# Print search terms and their counts\n",
    "print(\"Search terms and their matching article counts:\")\n",
    "sorted_search_terms = sorted(search_terms, key=lambda x: len(article_ids_by_search_term[x]), reverse=True)\n",
    "total_search_term_counts = 0\n",
    "for search_term in sorted_search_terms:\n",
    "\tprint(f\"- '{search_term}': {len(article_ids_by_search_term[search_term])}\")\n",
    "\ttotal_search_term_counts += len(article_ids_by_search_term[search_term])\n",
    "print(\"\")\n",
    "print(f\"Number of articles containing any of the search terms: {len(joint_article_ids)} ({len(joint_article_ids) / len(articles) * 100:.2f}% of {len(articles)} articles)\")\n",
    "print(f\"Total count of search term matches: {total_search_term_counts}\")\n",
    "print(f\"Average search terms per matching article: {total_search_term_counts / len(joint_article_ids):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# window_size = 20\n",
    "window_size = 30 * 3\n",
    "\n",
    "def get_df_of_article_counts_by_date(article_ids: set) -> pd.DataFrame:\n",
    "\t# Get df of article counts by date where the article contains any of the search terms using joint_article_ids set\n",
    "\tdf = pd.DataFrame()\n",
    "\tdf[\"date\"] = [datetime.strptime(articles[article_id][\"dateCreated\"].split(\" \")[0], \"%Y-%m-%d\") for article_id in article_ids]\n",
    "\tdf = df.set_index(\"date\")\n",
    "\t\n",
    "\t# Sort by date\n",
    "\tdf = df.sort_index()\n",
    "\t\n",
    "\t# Get counts\n",
    "\tdf = df.resample(\"D\").size() # resample to daily frequency\n",
    "\n",
    "\t# Add all missing dates from 2019-01-01 to 2023-12-31 if they are not already in the df index and set count to 0\n",
    "\tstart_date = \"2019-01-01\"\n",
    "\tend_date = \"2023-12-31\"\n",
    "\tall_dates = pd.date_range(start=start_date, end=end_date)\n",
    "\tdf = df.reindex(all_dates, fill_value=0)\n",
    "\n",
    "\t# make a new column with count\n",
    "\tdf = df.reset_index()\n",
    "\tdf.columns = [\"date\", \"count\"]\n",
    "\tdf = df.set_index(\"date\")\n",
    "\n",
    "\t# Get SMA\n",
    "\n",
    "\tdf[\"sma\"] = df[\"count\"].rolling(window_size).mean()\n",
    "\n",
    "\t# # Get EMA\n",
    "\tdf[\"ema\"] = df[\"count\"].ewm(span=window_size).mean()\n",
    "\n",
    "\t# Comment these two out to get daily data (in-depth exploration why some days have so many articles)\n",
    "\t# resample to other frequencies\n",
    "\t# df = df.resample(\"M\").sum()\n",
    "\tdf = df.resample(\"W\").sum()\n",
    "\t# remove day component from index (if \"M\")\n",
    "\t# df.index = df.index.map(lambda x: x.strftime(\"%Y-%m\"))\n",
    "\n",
    "\treturn df\n",
    "\n",
    "df = get_df_of_article_counts_by_date(joint_article_ids)\n",
    "\n",
    "# df.head()\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the number of articles containing the word for each date as a line chart\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# fig.add_trace(go.Scatter(\n",
    "# \tx=df.index,\n",
    "# \ty=df[\"count\"],\n",
    "# \tname=\"Count\",\n",
    "# ))\n",
    "\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "\tx=df.index,\n",
    "\ty=df[\"count\"],\n",
    "\tname=\"Count\",\n",
    "))\n",
    "\n",
    "# fig.add_trace(go.Scatter(\n",
    "# \tx=df.index,\n",
    "# \ty=df[\"sma\"],\n",
    "# \tname=f\"SMA-{window_size}\",\n",
    "# ))\n",
    "\n",
    "# fig.add_trace(go.Scatter(\n",
    "# \tx=df.index,\n",
    "# \ty=df[\"ema\"],\n",
    "# \tname=f\"EMA-{window_size}\",\n",
    "# ))\n",
    "\n",
    "def break_up_title(title: str, characters_per_line: int) -> str:\n",
    "\twords = title.split(\" \")\n",
    "\tlines = []\n",
    "\tline = \"\"\n",
    "\tfor word in words:\n",
    "\t\tif len(line) + len(word) + 1 <= characters_per_line:\n",
    "\t\t\tif len(line) > 0:\n",
    "\t\t\t\tline += \" \"\n",
    "\t\t\tline += word\n",
    "\t\telse:\n",
    "\t\t\tlines.append(line)\n",
    "\t\t\tline = word\n",
    "\tif len(line) > 0:\n",
    "\t\tlines.append(line)\n",
    "\treturn \"<br>\".join(lines)\n",
    "\n",
    "fig.update_layout(\n",
    "\t# title=f\"Weekly number of articles containing any of the search terms {search_terms}\",\n",
    "\ttitle = break_up_title(f\"Weekly number of articles containing any of the search terms: {search_terms}\", 100),\n",
    "\txaxis_title=\"Date\",\n",
    "\tyaxis_title=\"Count\",\n",
    ")\n",
    "\n",
    "# Make it 1500 x 1000\n",
    "fig.update_layout(\n",
    "\twidth=1000,\n",
    "\theight=400,\n",
    "\t# height=600,\n",
    ")\n",
    "\n",
    "# Margins\n",
    "fig.update_layout(\n",
    "\tmargin=dict(l=50, r=30, t=70, b=10),\n",
    "\t# margin=dict(l=50, r=30, t=270, b=10),\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "plot_name = \"article-counts-weekly-covid\"\n",
    "# plot_name = \"article-counts-monthly-covid\"\n",
    "# plot_name = \"article-counts-weekly-chip-shortage\"\n",
    "# plot_name = \"article-counts-monthly-chip-shortage\"\n",
    "# plot_name = \"article-counts-weekly-chip-shortage-big\"\n",
    "# plot_name = \"article-counts-monthly-chip-shortage-big\"\n",
    "# plot_name = \"article-counts-monthly-crypto\"\n",
    "# save as png with 3x scale\n",
    "fig.write_image(os.path.join(path_output_root, f\"{plot_name}.png\"), scale=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO weight by the number of daily articles ?\n",
    "\n",
    "# TODO: plot multiple search terms on the same chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print search terms and their counts\n",
    "print(\"Search terms and their matching article counts:\")\n",
    "sorted_search_terms = sorted(search_terms, key=lambda x: len(article_ids_by_search_term[x]), reverse=True)\n",
    "total_search_term_counts = 0\n",
    "for search_term in sorted_search_terms:\n",
    "\tprint(f\"- '{search_term}': {len(article_ids_by_search_term[search_term])}\")\n",
    "\ttotal_search_term_counts += len(article_ids_by_search_term[search_term])\n",
    "print(\"\")\n",
    "print(f\"Number of articles containing any of the search terms: {len(joint_article_ids)} ({len(joint_article_ids) / len(articles) * 100:.2f}% of {len(articles)} articles)\")\n",
    "print(f\"Total count of search term matches: {total_search_term_counts}\")\n",
    "print(f\"Average search terms per matching article: {total_search_term_counts / len(joint_article_ids):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_search_term_counts_for_date(article_ids_by_search_term: dict, date: str):\n",
    "\tprint(f\"Search terms and their matching article counts for date '{date}':\")\n",
    "\tsearch_term_counts = {}\n",
    "\tfor search_term in search_terms:\n",
    "\t\tcount = 0\n",
    "\t\tfor article_id in article_ids_by_search_term[search_term]:\n",
    "\t\t\tif articles[article_id][\"dateCreated\"].split(\" \")[0] == date:\n",
    "\t\t\t\tcount += 1\n",
    "\t\tsearch_term_counts[search_term] = count\n",
    "\tsorted_search_terms = sorted(search_terms, key=lambda x: search_term_counts[x], reverse=True)\n",
    "\tfor search_term in sorted_search_terms:\n",
    "\t\tprint(f\"- '{search_term}': {search_term_counts[search_term]}\")\n",
    "\tprint(\"\")\n",
    "\tprint(f\"Total search term counts for date '{date}': {sum(search_term_counts.values())}\")\n",
    "\t\n",
    "\n",
    "# Print search term counts for a specific date\n",
    "print_search_term_counts_for_date(article_ids_by_search_term, \"2022-03-04\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_articles_for_search_term_date(search_term: str, date: str):\n",
    "\tarticle_ids = article_ids_by_search_term[search_term]\n",
    "\tarticle_ids = [article_id for article_id in article_ids if articles[article_id][\"dateCreated\"].split(\" \")[0] == date]\n",
    "\tprint(f\"Search term '{search_term}' appears in {len(article_ids)} articles on {date}\")\n",
    "\tfor article_id in article_ids:\n",
    "\t\tprint(f\"- {article_id}: {articles[article_id]['title']}\")\n",
    "\n",
    "print_articles_for_search_term_date(\"coronavirus\", \"2022-03-04\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(filepath: str) -> str:\n",
    "\twith open(filepath, \"r\") as f:\n",
    "\t\treturn f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_id = \"0f8b1479-04b7-467c-ad94-ed843b90505f\"\n",
    "\n",
    "article_text = load_file(os.path.join(path_text_root, f\"{article_id}.txt\"))\n",
    "# article_text = article_text.replace(\"\\n\\n\", \"\\n\")\n",
    "article_text = article_text.replace(\"\\n\", \" \")\n",
    "\n",
    "words_per_line = 50\n",
    "print(\"Article text:\")\n",
    "print(break_up_title(article_text, words_per_line).replace(\"<br>\", \"\\n\"))\n",
    "print(\"\\n\\n\\n\")\n",
    "print(\"Article tokens:\")\n",
    "print(break_up_title(\" \".join(articles_tokens[article_id]), words_per_line).replace(\"<br>\", \"\\n\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
